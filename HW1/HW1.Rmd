---
title: "DATA 621 Homework 1"
author: "Critical Thinking Group 1"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: 6
  html_document:
    toc: yes
    toc_depth: '2'
    smart: no
  word_document:
    toc: yes
    toc_depth: '2'
---
\pagebreak
\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Prof. Dr. Nasrin Khansari\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
DATA 621 – Business Analytics and Data Mining\\
\medskip
Prepared by:\\
\medskip
Critical Thinking Group  1\\ 
\medskip
Vic Chan\\ 
\smallskip
Gehad Gad\\
\smallskip
Evan McLaughlin\\  
\smallskip
Bruno de Melo\\
\smallskip
Anjal Hussan\\
\smallskip
Zhouxin Shi\\
\end{center}

\pagebreak


```{r global_options, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, message = FALSE, warning = FALSE, echo=FALSE)
```

## Overview


In this homework assignment, we will explore, analyze and model a data set containing approximately 2200 records. Each record represents a professional baseball team from the years 1871 to 2006 inclusive. Each record has the performance of the team for the given year, with all of the statistics adjusted to match the performance of a 162 game season.

# Objective 

The objective is to build a multiple linear regression model on the training data to predict the number of wins for the team. we can only use the variables given to us (or variables that we derive from the variables provided). 

# Data Exploration


## Data Summary 

```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
##This block of code is to wrap codeblock in pdf/html output
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

```{r package, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
#Import required libraries
library(tidyr)
library(zoo)
library(pastecs)
library(dplyr)
library(ggplot2)
library(corrr)
library(GGally)
library(corrplot)
library(ggcorrplot)
library(reshape2)
library(graphics)
library(ggResidpanel)
library(gridExtra)
```



```{r results='hide', message=FALSE}
#Import the data
Data <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW1/moneyball-training-data.csv")
```

The dataset contain 17 columns and 2276 observations or records. The first column is the index which will be deleted as it has no use in the analysis. The target variable is the **TARGET_WINS** column. The dataset is all numerical and does include any categorical variables.

```{r results='hide', message=FALSE}
#Remove the index
Data1 <- Data [-c(1)]
```


```{r results='hide', message=FALSE}
#Check the Summary
summary(Data1)
```

At a glance we can see that the data a significant number of NA values and the average wins for a team is about 81.

````{r results='hide', message=FALSE, echo=FALSE}
#The mean for each column in the data
colMeans(Data1)
```

```{r results='hide', message=FALSE, echo=FALSE}
#The Standard Deviation for each column in the data
sapply(Data1, sd)
```

```{r results='hide', message=FALSE, echo=FALSE}
#The median for each column in the data
apply(Data1,2, median)
```

```{r results='hide', message=FALSE, echo=FALSE}
#Search if there are any NA values
sum(is.na(Data1))
```

```{r results='hide', message=FALSE, echo=FALSE}
#We are not able to delete the NA values. We will replace NA values.
Data2 = replace(Data1, TRUE, lapply(Data1, na.aggregate))
```

```{r results='hide', message=FALSE, echo=FALSE}
#Confirm the all NA values were replaced by the mean.
sum(is.na(Data2))
```


## Distribution
```{r}
Data2 %>%
  gather(var, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_density(color = "blue") + 
  facet_wrap(~var, scales= "free", ncol = 5)
```


The distribution of the target variable **TARGET_WINS** is normally distributed.

The distribution also show that **BASERUN_SB** and **BATTING_3B** are right skewed, and additional **BATTING_HR** and **PITCHING_HR** are bimodal distributions.

## Boxplot
```{r}
Data2 %>%
  gather(var, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_boxplot(notch = TRUE) + 
  facet_wrap(~var, scales= "free", ncol = 4)
```

The box-plots above give us idea about the spread of each variable in the data which reveal significant outliers in a lot of the columns.

## Correlation

```{r echo=FALSE}
# Use pearson correlation
Data2 %>% correlate() %>% focus(TARGET_WINS)
```

```{r}
ggcorr(Data2)
```

```{r}
#Add correlation coefficients
corr <- round(cor(Data2), 1)
ggcorrplot(corr, hc.order = TRUE, type = "lower",
   lab = TRUE)
```

These correlations plots do not show a strong relationship between any two variables. This indicate presence of ‘noise’ in these relationships. It is interesting to note that allowing hits have little positive impacts on wins. 
It is also noteworthy that pitching strikeouts by batters and hits allowed are negatively correlated with winning. I assume the correlations are affected by the outliers.


# Data Preparation

In this section we will be looking at the different ways to prepare the data for modeling. We will show the different steps that we took and the reasoning on why we did certain transformations, replacement and creation of columns.

```{r importing data set}
moneyball_training_data = read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW1/moneyball-training-data.csv")
```


## Finding All NA
```{r finding all NA}
na_count = sapply(moneyball_training_data, function(y) sum(is.na(y)))
na_count = data.frame(na_count)
na_count %>%
  arrange(desc(na_count)) %>%
  mutate(total_rows = nrow(moneyball_training_data)) %>%
  mutate(percent_missing = na_count / total_rows)
```
Initially when looking at the data we can see that **TEAM_BATTING_HBP** is missing 91% of its data and **TEAM_BASERUN_CS** is missing around 34% of its data. This is a lot of data missing which is why those columns will be removing these. Based on different studies there is no definite percentage of for how much data one should be missing before removing the column, but it is always better to have more data. The columns **TEAM_FIELDING_DP**, **TEAM_BASERUN_SB**, **TEAM_BATTING_SO**, and **TEAM_PITCHING_SO** are missing around 12% - 4% of its data and can fill those in with using mean and median. In the next section we will look at to see whether using the mean or median would be the better choice in filling the missing data.

```{r removing column}
moneyball_subset = subset(moneyball_training_data, select=-c(TEAM_BATTING_HBP, TEAM_BASERUN_CS, INDEX))
```


## Replacing NA with Mean or Median

In this section we will need to decide whether to fill the missing data using the mean or median. We will need to look at the distribution of each of the columns with missing data in order to decide if we will be using the median or mean to fill in the missing data.
     

```{r}
missing_data = subset(moneyball_subset, select = c(TEAM_FIELDING_DP, TEAM_BASERUN_SB, TEAM_BATTING_SO, TEAM_PITCHING_SO))
missing_data = melt(missing_data)

ggplot(missing_data, aes(x = value)) + geom_histogram(binwidth = 10) + facet_wrap(~variable, scale='free')
```
     
Looking at the above graphs we can see that not all the distribution are uniform distribution. We can see that **TEAM_BATTING_SO** is a bimodal distribution, **TEAM_BASERUN_SB** is skewed to the right, and **TEAM_PITCHING_SO** has very large outliers. For this reason we will be using the median to replace all the missing data as the median is less susceptible to outliers and non-uniform distributions.

```{r}
replace_na_with_median = function(x){
  x[is.na(x)] = median(x, na.rm=TRUE)
  return(x)
}

moneyball_fill = apply(moneyball_subset, 2, replace_na_with_median)
moneyball_fill = as.data.frame(moneyball_fill)
```



## Transformation
We will also be needing to check all of the columns to see if they will need any type of transformation in order to create a linear line. We will be be graphing all the columns with **TARGET_WINS** as the response variable. This will allow us to see if there are any columns that can be transformed in order to improve the model.      
       
```{r}
par(mfrow=c(2, 3))


for (i in 2:ncol(moneyball_fill)){
  
  y = moneyball_subset[,1]
  x = moneyball_subset[,i]
  plot(
    x, 
    y,   
    ylab = 'TARGET_WINS',
    xlab = names(moneyball_fill)[i]
  )
}
```
     
Looking at the graphs above we can see that none of the columns are real good candidates for transformation.


## Putting Teams Into Buckets

We will be putting the dataset into buckets based on the teams winning score as this will allow us to see if there is any patterns between weak and strong teams. The teams will be split into two groups **Strong** and **Weak** based on the **TARGET_WINS** column. The maximum **TARGET_WINS** is 146 and the minimum **TARGET_WINS** is 0 therefore the split is 73.

```{r}
moneyball_fill$TEAM_TYPE = cut(moneyball_subset[,'TARGET_WINS'], breaks=c(0, 73, 146), include.lowest = TRUE, labels = c('Weak', 'Strong'))
```


## Creating Total Hits

We needed to create a column which include all the different batting statistics. By combining **TEAM_BATTING_H**, **TEAM_BATTING_2B**, **TEAM_BATTING_3B** AND **TEAM_BATTING_HR** we are able to measure the total amount bases each team scored. The reason why we needed this column is because there can be teams that score more home runs than single bases. Combining this information all into one column will make it easier to build a model as we will not need to put as many variables.
```{r}
createTEAM_BATTING_TOTAL = function(x){
  x$TEAM_BATTING_TOTAL = (x$TEAM_BATTING_H 
                          + (2 * x$TEAM_BATTING_2B) 
                          + (3 * x$TEAM_BATTING_3B) 
                          + (4 * x$TEAM_BATTING_HR))
  return(x)
}
```

```{r}
moneyball_fill$TEAM_BATTING_TOTAL = (moneyball_fill$TEAM_BATTING_H + (2 * moneyball_fill$TEAM_BATTING_2B) + (3 * moneyball_fill$TEAM_BATTING_3B) + (4 * moneyball_fill$TEAM_BATTING_HR))
```

```{r}
ggplot(moneyball_fill, aes(x=TEAM_BATTING_TOTAL, y=TARGET_WINS)) + geom_smooth(method='lm') + geom_point(aes(color=TEAM_TYPE))
```

## Base Percentage
We would like to create a column which measures the total amount of bases a team get per game. This will be calculated by using the new created column **TEAM_BATTING_TOTAL** dividing 162 game season.    

```{r}
moneyball_fill$TEAM_BATTING_PERCENT =  moneyball_fill$TEAM_BATTING_TOTAL / 162
```

```{r}
ggplot(moneyball_fill, aes(x=TEAM_BATTING_PERCENT, y=TARGET_WINS)) + geom_smooth(method='lm') + geom_point(aes(color=TEAM_TYPE))
```

# Model Building

At the beginning, we were presented with 16 independent variables. It makes sense to exclude index since it is not relevant. It also makes sense to exclude **TEAM_BATTING_HBP** and **TEAM_BASERUN_CS** since they are comprised of so many N/As. We are thus able to concentrate on the 13 remaining variables, pursuing continuous incremental model improvement.

To start with, our first three models are outlined below.       
       
  lmodel1 - an "all-in" model that includes all 13 remaining variables     
  lmodel2 - a model that strips out outliers      
  lmodel3 - a model that eliminates impertinent attributes

  
```{r echo=FALSE, results='hide'}
names(moneyball_fill) <- tolower(names(moneyball_fill))
#let's strip out the team type since it doesn't enhance the model
train1 <- subset(moneyball_fill, select = -c(team_type))
head(train1)
```

We'll start with the all-in model
```{r}

lmodel1 <- lm(target_wins ~ ., data = train1)
summary(lmodel1)
```
So in looking at the all-in model, we can identify how the model behaves intuitively and not-so-intuitively. For example, we see the following variables as having positive coefficients: **TEAM_BATTING_H**, **TEAM_BATTING_3B**, **TEAM_BASERUN_SB**, and **BEAM_PITCH_STRIKEOUT**. These make sense, as you'd expect a team to win games that gets hits, hits triples, steals bases efficiently, and strikes out opponents. However, some of the positive coefficients don't make as much sense. For example, we would expect teams whose pitchers give up lots of home runs to not win very many games. This certainly warrants further analysis.

For negative coefficients, we'd obviously expect teams whose players make a lot of errors to not win at a high rate. However, hitting doubles and fielding double plays have negative coefficients as well, which are not intuitive at all.

A majority of the variables that we are assessing appear to contribute to predicting wins. We can gain some comfort in our model due to the low RSE (13.07) and satisfactory F-statistic (80.1), and we should feel ok about the overall efficacy of our model. However, the Adjusted R-square well under 1 is cause for some concern, but we can look to improve that in future iterations of the model. 

What else can we do to improve our model? Well, its predictive value might be enhanced by eliminating some problematic outliers. So let's take a look at if it makes sense to do so.      
```{r}
res1 <- resid(lmodel1)
plot(fitted(lmodel1), res1)

abline(0,0)

qqnorm(res1, pch = 1, frame = FALSE)
```
The data is not evenly scattered but we don't detect any unexpected non-linear pattern. The normal QQ looks good as well with a relatively straight line. We can spot some outliers that we should drill down on using Cook's Distance. Then, we can then attempt to strip them out to improve our model somewhat.      

```{r}
cooksd <- cooks.distance(lmodel1)
sample_size <- nrow(train1)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4/sample_size, col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4/sample_size, names(cooksd),""), col="red")
```
We can spot two that breach our threshold, so now we set about removing them. Next, we can re-run our initial all-in model to see if dropping the outliers has any impact on improving the model.

```{r}
influential <- as.numeric(names(cooksd)[(cooksd > (4/sample_size))])
train1_strip <- train1[-influential, ]

lmodel2 <- lm(target_wins ~ ., data = train1_strip)
summary(lmodel2)
```
This looks like good news. Our RSE is down, and our F-statistic is up. Even our Adjusted R-Squared value is up slightly from .31. Nevertheless, the explanatory value of our model remains limited without this last number increasing significantly. And we can clearly see some variables with high p-values that ought to be removed in order to improve our model. Let's proceed with removing `team_batting_hr` and `team_picthing_hr`.

```{r}

train3 <- subset(train1_strip, select = -c(team_batting_hr,team_pitching_hr))
lmodel3 <- lm(target_wins ~ ., data = train3)
summary(lmodel3)
```

We've improved the model incrementally by removing variables with high p-values, and our RSE and F-stat look better The explanatory power of our model, however, remains in doubt due to the Adjusted R-Squared value that remains low, even though it's improved from the previous model. What stands out here is that triples hit, bases stolen, and gaining walks remain the overall strongest positive coefficients, while `team_fielding_dp` remains the largest negative coefficient, which is counter-intuitive at first blush. However, one thing necessary for a double play is at least one opponent runner on base. Those teams that earn a high number of double plays are only able to do so because their pitchers are allowing runners on base to begin with. 


In addition to the aforementioned three models, we will be analysing two additional models as follows:   
    
    1: lmodel4 - model containing three variables which we believe are more related to winning games:
    'team_batting_total` + `team_pitching_h` + `team_batting_3b`

    2: lmodel5 - a model put together by someone who does not understand the game of baseball;
       
We will fit a model on two variables which we believe are more related to winning games:
'team_batting_total` + `team_pitching_h`.

```{r}
lmodel4<-lm(target_wins ~ team_batting_total + team_pitching_h, data = train1)
summary(lmodel4)
```
Adjusted R-squared is lower then what we have in model3. As for interpretation, batting total has a positive impact on wins while hits allowed have a negative impact, as expected.

Residual plots are below:


```{r}
par(mfrow = c(2, 2))  # Split the plotting panel into a 2 x 2 grid
plot(lmodel4)  # Plot the model information
```

Now we will do a random model. The subject matter *non-expert* does not understand batters, pitchers, walks, double plays, etc. What (s)he knows is that in every game, not losing is related to a low number of errors - so the variable 'team_fielding_e' is picked and also (s)he thinks that stolen is not good so the variable `team_baserun_sb` is also chosen .
```{r}

lmodel5 <- lm(target_wins ~ team_baserun_sb + team_fielding_e, data = train1)
summary(lmodel5)
```
As expected, such a random model does not yield very good results, despite all coefficients being statistically different from zero. As we can see from the very low R-squared and high RSE. In terms of interpretation, number of stolen bases have a positive impact on winnings, contradicting what the *non-expert* thought and number of errors has a negative impact on winnings, which is intuitive, as expected by our *non-expert*.

In addition, analyzing the residual plots we see that the residuals are not normally distributed and present heterocedasticity.

```{r test}
par(mfrow = c(2, 2))  # Split the plotting panel into a 2 x 2 grid
plot(lmodel5)  # Plot the model information
```

# Select Models

## Model comparison
Now, before we select our model, let's find out the statistics of our models so that we can compare the models using the indicators i.e. R2, MSE, F-statistic(f), Number of Variables (k), Number of Observations (n)

```{r echo=FALSE, message=FALSE}
results = NULL
models = list(model1 = lmodel1, 
              model2 = lmodel2,
              model3 = lmodel3,
              model4 = lmodel4,
              model5 = lmodel5)

for (i in names(models)){
  s = summary(models[[i]])
  name = i
  mse = mean(s$residuals^2)
  r2 = s$r.squared
  pvalue = s$coefficients[[8]] 
  adjustedR2 = s$adj.r.squared
  f = s$fstatistic[1]
  k = s$fstatistic[2]
  n = s$fstatistic[3]
  removedObservations = nrow(train1)-n
  results = rbind(results, data.frame(name = name, 
                                      rsquared = r2,
                                      adjustedR2  = adjustedR2,
                                      mse = mse,
                                      f = f,
                                      pvalue = pvalue,
                                      k = k,
                                      n = n
                                      )
                  )
}

rownames(results) = NULL
knitr::kable(results)
```


In order to select on model we need to consider few aspects of the models:

## R-squared:
R-squared evaluates the scatter of the data points around the fitted regression line. It is also called the coefficient of determination. For the same data set, higher R-squared values represent smaller differences between the observed data and the fitted values.

R-squared is the percentage of the dependent variable variation that a linear model explains.

R-squared is always between 0 and 100%. 0% represents a model that does not explain any of the variation in the response variable around its mean. The mean of the dependent variable predicts the dependent variable as well as the regression model.
100% represents a model that explains all the variation in the response variable around its mean.
Usually, the larger the R2, the better the regression model fits your observations.

Among the models, the r-Squared of Model2 and Model3 are almost similar and has highest R2. Based on R2, we can select model2 and model3 as our best model candidate.

## Adjusted R-squared:
The adjusted R-squared is a modified version of R-squared that adjusts for predictors. when we compare Adj-R-Squared among the input variables, the lower adjusted R-squared indicates that the additional input variables are not adding value to the model. A higher adjusted R-squared indicates that the changes in input variables are adding value to the model.
Among the models, model3 has the highest adjusted R2 which indicates that model3 could be the best model. The model4 and model5 have the lowest adjusted R2 compare to the first three models.

## Residuals:
Residuals are estimates of experimental error obtained by subtracting the observed responses from the predicted responses. The predicted response is calculated from the chosen model. Since this is a form of error, the same general assumptions apply to the group of residuals that we typically use for errors in general: one expects them to be (roughly) normal and (approximately) independently distributed with a mean of 0 and some constant variance.in other words, we should not see any pattern in the residuals when plotting. A simple plot is suitable for displaying the normality of the distribution of a group of residuals

Using ggResidpanel package, we can quickly visualize residuals from all models. In this visuals, the first plot is for model1, second plot is for model2 and so on.

```{r residualCompare}
resid_compare(models, plots = c("resid", "hist"))
```

From the plots and histogram, it looks like the residuals from model2 and model3 have no obvious pattern. residuals from model4 and model 5 are left skewed.Hence we can select model2 and model3 as the candidate for the best model

## p-value:
we need to evaluate p-value and if the p-value is much less than 0.05 then we can reject the null hypothesis. That will indicate that there is a significant relationship between the variables in the linear regression model of the data set. Based on the p-value shown in the table, we can pick model1, model2, and model3 as best model candidate. p-value of the model4 and model5 are not significant. Hence we can not select model4 and model5 as our best model

Although model2 and model3 are the best candidates. Both models have highest R2 and increasing Adjusted R2. p-value for both of the models are significant. But we are going to select model3 as our final model based on the statistics and diagnostics especially adjusted R2 which is slightly higher and the p-value which is low in comparison with model2

## Final Model Review
Now that we have selected model3 as our best model, let's take a look to some other aspects of our final model.

The dependents columns of our selected model are:
```{r echo=FALSE}
print(names(train3))
```

Let's review the diagnostic plots and a plot of the residuals.  

```{r, echo=FALSE, message=FALSE, warning=FALSE}
plot(lmodel3)
```

## Plot the residual of selected model

```{r, echo=FALSE, message=FALSE, warning=FALSE}
plot(lmodel3$residuals)
```
The residual plot looks perfect and there is no obvious pattern noticed in this plot

## Create histogram of the residuals of selected model
```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data = train3, aes(x = lmodel3$residuals)) +
    geom_histogram(fill = 'steelblue', color = 'black') +
    labs(title = 'Histogram of Residuals', x = 'Residuals', y = 'Frequency')
```

From this histogram we can visualize that the distribution is normal. 

## Plot the top Coefficients of our model

```{r, echo=FALSE, message=FALSE}
coef <- data.frame(sort(lmodel3$coefficients))
coef$names <- rownames(coef)
names(coef) <- c("coef","names")
imp_coef <- rbind(head(coef,10),
                  tail(coef,10))
imp_coef$coef <- scale(imp_coef$coef)
ggplot(imp_coef) +
    geom_bar(aes(x=reorder(names,coef),y=coef),
             stat="identity") +
    coord_flip() +
    ggtitle("Most Important 20 Coefficents \n in our Final Model (Scaled)") +
    theme(axis.title=element_blank(), plot.title=element_text(hjust=0.5))
```

# Predictions using evaluation data

Let's use our evaluation data to predict and evaluate our selected model.
 
```{r predict, echo=FALSE, message=FALSE, warning=FALSE}
eval_data <- read.csv('https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW1/moneyball-evaluation-data.csv', header=T)

#create Team Batting total in the evaluation data
eval_data = createTEAM_BATTING_TOTAL(eval_data)

#Add percent column to match with the model
eval_data$TEAM_BATTING_PERCENT =  eval_data$TEAM_BATTING_TOTAL / 162

#convert header to lower case to match with the models
names(eval_data) <- tolower(names(eval_data))

predictiondf <- predict(lmodel3, eval_data)
predictiondf <- data.frame(predictiondf)

write.csv(predictiondf, "./testPredictions.csv")
```

```{r}
res3 <- resid(lmodel3)
plot(fitted(lmodel3), res3)
abline(0,0)
```

Similar to the training data, the evaluation data also needs some prep work.The NA values imputed used the same method as what we done before. We successfully predicted the number of wins with our selected model3 which has a higher R2 and F-statistic. But the residuals are unevenly dispersed relative to the fitted values, indicating that the variance of the residuals is not constant. And the model lacks a higher order term for one variable to explain the curvature.


### Compare predicted to original distribution

```{r, echo=FALSE, message=FALSE}
p1 <- ggplot(train3, aes(target_wins)) + geom_histogram() + ggtitle("Training Win Distribution") 
p2 <- ggplot(predictiondf, aes(predictiondf)) + geom_histogram() + ggtitle("Predicted Win Distribution")
grid.arrange(p1, p2, ncol=2)
```

The Training win distribution and predicted win distribution look similar.

\pagebreak

# Appendix

* For full output code visit: https://github.com/ahussan/DATA_621_Group1/blob/main/HW1/HW1.Rmd
* For predicted values over test set visit:  https://github.com/ahussan/DATA_621_Group1/blob/main/HW1/testPredictions.csv



