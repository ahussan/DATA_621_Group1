---
title: "DATA 621 Homework 1"
author: "Critical Thinking Group 1"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: 6
  html_document:
    toc: yes
    toc_depth: '2'
    smart: no
  word_document:
    toc: yes
    toc_depth: '2'
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, message = FALSE, warning = FALSE)
```

## Overview
\pagebreak
\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Prof. Dr. Nasrin Khansari\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
DATA 621 – Business Analytics and Data Mining\\
\medskip
Prepared by:\\
\medskip
Critical Thinking Group  1\\ 
\medskip
Vic Chan\\ 
\smallskip
Gehad Gad\\
\smallskip
Evan McLaughlin\\  
\smallskip
Bruno de Melo\\
\smallskip
Anjal Hussan\\
\smallskip
Zhouxin Shi\\
\end{center}

\pagebreak
# Overview

In this homework assignment, we will explore, analyze and model a data set containing approximately 2200 records. Each record represents a professional baseball team from the years 1871 to 2006 inclusive. Each record has the performance of the team for the given year, with all of the statistics adjusted to match the performance of a 162 game season.

# Objective 

The objective is to build a multiple linear regression model on the training data to predict the number of wins for the team. we can only use the variables given to us (or variables that we derive from the variables provided). 

#Data Exploration


### Data Summary 

```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
##This block of code is to wrap codeblock in pdf/html output
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

```{r package, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
#Import required libraries
library(tidyr)
library(zoo)
library(pastecs)
library(dplyr)
library(ggplot2)
library(corrr)
library(GGally)
library(corrplot)
library(ggcorrplot)
library(reshape2)
library(graphics)
```



```{r}
#Import the data
Data <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW1/moneyball-training-data.csv")
head(Data)
```

We can see that the data contain 17 columns and 2276 observations or records. The first column is the index which will be deleted as it is not useful. The target variable is the "TARGET_WINS". Notice that the dateset is numerical and does not have any categorical variables.

```{r}
#Remove the index
Data1 <- Data [-c(1)]
```


```{r}
#Check the Summary
summary(Data1)
```

Summary of the data gives a useful information about each variable. 

- There is a significant number of NA  values. 

- The average wins for a team is about 81.


````{r}
#The mean for each column in the data
colMeans(Data1)
```

```{r}
#The Standard Deviation for each column in the data
sapply(Data1, sd)
```

```{r}
#The median for each column in the data
apply(Data1,2, median)
```


### Missing Values


```{r}
#Search if there are any NA values
sum(is.na(Data1))
```

```{r}
#We are not able to delete the NA values. We will replace NA values.
Data2 = replace(Data1, TRUE, lapply(Data1, na.aggregate))
```

```{r}
#Confirm the all NA values were replaced by the mean.
sum(is.na(Data2))
```


### Distribution
```{r}
Data2 %>%
  gather(var, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_density(color = "blue") + 
  facet_wrap(~var, scales= "free", ncol = 5)
         
```


The distribution of the target variable "target_wins" column is normally distributed and there are many average teams.

The distribution also show that BASERUN_SB and BATTING 3B are right skew variables. In addition, BATTING_HR and PITCHING_HR are bimodal.

### Boxplot
```{r}
Data2 %>%
  gather(var, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_boxplot(notch = TRUE) + 
  facet_wrap(~var, scales= "free", ncol = 4)
```

The box-plots above give us idea about the spread of each variable in the data. The box-plots reveal significant outliers.

### Correlation

```{r}
# Use pearson correlation
Data2 %>% correlate() %>% focus(TARGET_WINS)

```

These correlations do not show a strong relationship. This indicate presence of ‘noise’ in these relationships.
It is interesting to note allowing hits have little positive impacts on wins. 
It is also noteworthy that pitching strikeouts by batters and hits allowed are negatively correlated with winning. I assume the correlations are affected by the outliers.


```{r}
ggcorr(Data2)
```

```{r}
#Add correlation coefficients
corr <- round(cor(Data2), 1)
ggcorrplot(corr, hc.order = TRUE, type = "lower",
   lab = TRUE)
```





# Data Preparation

In this section we will be looking at the different ways to prepare the data for modeling. We will show the different steps that we took and the reasoning why we did certain transformations, replacement and creation of columns.

```{r importing data set}
moneyball_training_data = read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW1/moneyball-training-data.csv")
```


```{r finding all NA}
na_count = sapply(moneyball_training_data, function(y) sum(is.na(y)))
na_count = data.frame(na_count)
na_count %>%
  arrange(desc(na_count)) %>%
  mutate(total_rows = nrow(moneyball_training_data)) %>%
  mutate(percent_missing = na_count / total_rows)
```
Initially when looking at the data we can see that **TEAM_BATTING_HBP** is missing 91% of its data and **TEAM_BASERUN_CS** is missing around 34% of its data. This is a lot of data missing which is why those columns will be removing these. Based on online reading there is no definite cut of for how much data one should be missing before removing a column, but it is always better to have more data. The columns **TEAM_FIELDING_DP**, **TEAM_BASERUN_SB**, **TEAM_BATTING_SO**, and **TEAM_PITCHING_SO** are missing around 12% - 4% of its data and can fill those in with using mean and median. In the next section we will look at to see whether using the mean or median would be the better choice in filling the missing data.

```{r removing column}
moneyball_subset = subset(moneyball_training_data, select=-c(TEAM_BATTING_HBP, TEAM_BASERUN_CS, INDEX))
```


## Replacing NA with Mean or Median

In this section we will need to decide whether to fill the missing data using the mean or median. We will need to look at the distribution of each of the columns with missing data in order to decide if we will be using the median or mean to fill in the missing data.
     

```{r}
missing_data = subset(moneyball_subset, select = c(TEAM_FIELDING_DP, TEAM_BASERUN_SB, TEAM_BATTING_SO, TEAM_PITCHING_SO))
missing_data = melt(missing_data)

ggplot(missing_data, aes(x = value)) + geom_histogram(binwidth = 10) + facet_wrap(~variable, scale='free')
```
     
Looking at the above graphs we can see that not all the distribution are uniform distribution. We can see that **TEAM_BATTING_SO** is a bimodal distribution, **TEAM_BASERUN_SB** is skewed to the right, and **TEAM_PITCHING_SO** has very large outliers. For this reason we will be using the median to replace all the missing data as the median is less susceptible to outliers and non-uniform distributions.

```{r}
replace_na_with_median = function(x){
  x[is.na(x)] = median(x, na.rm=TRUE)
  return(x)
}

moneyball_fill = apply(moneyball_subset, 2, replace_na_with_median)
moneyball_fill = as.data.frame(moneyball_fill)
```



## Transformation
We will also be needing to check all of the columns to see if they will need any type of transformation in order to create a linear line. We will be be graphing all the columns with **TARGET_WINS** as the response variable. This will allow us to see if there are any columns that can be transformed in order to improve the model.      
       
```{r}
par(mfrow=c(2,2))


for (i in 2:ncol(moneyball_fill)){
  
  y = moneyball_subset[,1]
  x = moneyball_subset[,i]
  plot(
    x, 
    y,   
    ylab = 'TARGET_WINS',
    xlab = names(moneyball_fill)[i]
  )
}
```
     
Looking at the graphs above we can see that none of the columns are real good candidates for transformation.


## Putting Teams Into Buckets

We will be putting the dataset into buckets based on the teams winning score as this will allow us to see if there is any patterns between weak and strong teams. The teams will be split into two groups **Strong** and **Weak** based on the **TARGET_WINS** column. 

```{r}
moneyball_fill$TEAM_TYPE = cut(moneyball_subset[,'TARGET_WINS'], breaks=c(0, 73, 146), include.lowest = TRUE, labels = c('Weak', 'Strong'))
```


## Creating Total Hits
Creating a column which includes the total amount of hits a team has
```{r}
createTEAM_BATTING_TOTAL = function(x){
  x$TEAM_BATTING_TOTAL = (x$TEAM_BATTING_H 
                          + (2 * x$TEAM_BATTING_2B) 
                          + (3 * x$TEAM_BATTING_3B) 
                          + (4 * x$TEAM_BATTING_HR))
  return(x)
}
```

```{r}
moneyball_fill$TEAM_BATTING_TOTAL = (moneyball_fill$TEAM_BATTING_H + (2 * moneyball_fill$TEAM_BATTING_2B) + (3 * moneyball_fill$TEAM_BATTING_3B) + (4 * moneyball_fill$TEAM_BATTING_HR))
```

```{r}
ggplot(moneyball_fill, aes(x=TEAM_BATTING_TOTAL, y=TARGET_WINS)) + geom_smooth(method='lm') + geom_point(aes(color=TEAM_TYPE))
```

## Hit Percentage
We would like to create a column which states what is the teams hit/base they get per game. This will be calculated by summing the total amount of hits a team gets and dividing 162 game season.     

```{r}
moneyball_fill$TEAM_BATTING_PERCENT =  moneyball_fill$TEAM_BATTING_TOTAL / 162
```

```{r}
ggplot(moneyball_fill, aes(x=TEAM_BATTING_PERCENT, y=TARGET_WINS)) + geom_smooth(method='lm') + geom_point(aes(color=TEAM_TYPE))
```

## Model Building

At the beginning, we were presented with 16 independent variables. It makes sense to exclude index since it is not relevant. It also makes sense to exclude `team_batting_hbp` and `team_baserun_cs` since they are comprised of so many N/As. We are thus able to concentrate on the 13 remaining variables, pursuing continuous incremental model improvement.

To start with, our first three models are outlined below.       
       
  lmodel1 - an "all-in" model that includes all 13 remaining variables     
  lmodel2 - a model that strips out outliers      
  lmodel3 - a model that eliminates impertinent attributes

  
```{r}

names(moneyball_fill) <- tolower(names(moneyball_fill))
#let's strip out the team type since it doesn't enhance the model
train1 <- subset(moneyball_fill, select = -c(team_type))
head(train1)
```

We'll start with the all-in model
```{r}

lmodel1 <- lm(target_wins ~ ., data = train1)
summary(lmodel1)
```
So in looking at the all-in model, we can identify how the model behaves intuitively and not-so-intuitively. For example, we see the following variables as having positive coefficients: `team_batting_h`, `team_batting_3b`, `team_baserun_sb`, and `team_pitching_strikeouts`. These make sense, as you'd expect a team to win games that gets hits, hits triples, steals bases efficiently, and strikes out opponents. However, some of the positive coefficients don't make as much sense. For example, we would expect teams whose pitchers give up lots of home runs to not win very many games. This certainly warrants further analysis.

For negative coefficients, we'd obviously expect teams whose players make a lot of errors to not win at a high rate. However, hitting doubles and fielding double plays have negative coefficients as well, which are not intuitive at all.

A majority of the variables that we are assessing appear to contribute to predicting wins. We can gain some comfort in our model due to the low RSE (13.07) and satisfactory F-statistic (80.1), and we should feel ok about the overall efficacy of our model. However, the Adjusted R-square well under 1 is cause for some concern, but we can look to improve that in future iterations of the model. 

What else can we do to improve our model? Well, its predictive value might be enhanced by eliminating some problematic outliers. So let's take a look at if it makes sense to do so.      
```{r}
res1 <- resid(lmodel1)
plot(fitted(lmodel1), res1)

abline(0,0)

qqnorm(res1, pch = 1, frame = FALSE)
```
The data is not evenly scattered but we don't detect any unexpected non-linear patter. The normal QQ looks good as well with a relatively straight line. We can spot some outliers that we should drill down on using Cook's Distance. Then, we can then attempt to strip them out to improve our model somewhat.      

```{r}
cooksd <- cooks.distance(lmodel1)
sample_size <- nrow(train1)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4/sample_size, col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4/sample_size, names(cooksd),""), col="red")
```
We can spot two that breach our threshold, so now we set about removing them. Next, we can re-run our initial all-in model to see if dropping the outliers has any impact on improving the model.

```{r}
influential <- as.numeric(names(cooksd)[(cooksd > (4/sample_size))])
train1_strip <- train1[-influential, ]

lmodel2 <- lm(target_wins ~ ., data = train1_strip)
summary(lmodel2)
```
This looks like good news. Our RSE is down, and our F-statistic is up. Even our Adjusted R-Squared value is up slightly from .31. Nevertheless, the explanatory value of our model remains limited without this last number increasing significantly. And we can clearly see some variables with high p-values that ought to be removed in order to improve our model. Let's proceed with removing `team_batting_hr` and `team_picthing_hr`.

```{r}

train3 <- subset(train1_strip, select = -c(team_batting_hr,team_pitching_hr))
lmodel3 <- lm(target_wins ~ ., data = train3)
summary(lmodel3)
```

We've improved the model incrementally by removing variables with high p-values, and our RSE and F-stat look better The explanatory power of our model, however, remains in doubt due to the Adjusted R-Squared value that remains low, even though it's improved from the previous model. What stands out here is that triples hit, bases stolen, and gaining walks remain the overall strongest positive coefficients, while `team_fielding_dp` remains the largest negative coefficient, which is counter-intuitive at first blush. However, one thing necessary for a double play is at least one opponent runner on base. Those teams that earn a high number of double plays are only able to do so because their pitchers are allowing runners on base to begin with. 


In addition to the aforementioned three models, we will be analysing two additional models as follows:    

    1: lmodel_random - a model put together by someone who does not understand the game of baseball; and
    
    2: lmodel4 - model containing three variables which we believe are more related to winning games: 'team_batting_total` + `team_pitching_h` + `team_batting_3b`
       
    

We'll start with the random model. The subject matter *non-expert* does not understand batters, pitchers, walks, double plays, etc. What (s)he knows is that in every game, not losing is related to a low number of errors - so the variable 'team_fielding_e' is picked and also (s)he thinks that stolen is not good so the variable `team_baserun_sb` is also chosen .
```{r}

lmodel_random <- lm(target_wins ~ team_baserun_sb + team_fielding_e, data = train1)
summary(lmodel_random)
```    
As expected, such a random model does not yield very good results, despite all coefficients being statistically different from zero. As we can see from the very low R-squared and high RSE. In terms of interpretation, number of stolen bases have a positive impact on winnings, contradicting what the *non-expert* thought and number of errors has a negative impact on winnings, which is intuitive, as expected by our *non-expert*. 

In addition, analysing the residual plots we see that the residuals are not normally distributed and present heterocedasticity.

```{r test}
par(mfrow = c(2, 2))  # Split the plotting panel into a 2 x 2 grid
plot(lmodel_random)  # Plot the model information
```

Moving on to our expert model, we will fit a model on two variables which we believe are more related to winning games: 'team_batting_total` + `team_pitching_h`.

```{r}
lmodel4<-lm(target_wins ~ team_batting_total + team_pitching_h, data = train1)
summary(lmodel4)
```
Adjusted R-squared is better than our random model but still below that of model3. As for interpretation, batting total has a positive impact on wins while hits allowed have a negative impact, as expected.

Residual plots are below, and residuals are definitive more well behaved than in the random model.


```{r}
par(mfrow = c(2, 2))  # Split the plotting panel into a 2 x 2 grid
plot(lmodel4)  # Plot the model information
```
# Select Models
In order to select on model we need to consider few aspects of the models:

Degrees of Freedom: Number of observations minus the number of coefficients (including intercepts). The larger this number is the better and if it’s close to 0, your model is seriously over fit.

### R-squared:
R-squared evaluates the scatter of the data points around the fitted regression line. It is also called the coefficient of determination. For the same data set, higher R-squared values represent smaller differences between the observed data and the fitted values.

R-squared is the percentage of the dependent variable variation that a linear model explains.

R-squared is always between 0 and 100%:

0% represents a model that does not explain any of the variation in the response variable around its mean. The mean of the dependent variable predicts the dependent variable as well as the regression model.
100% represents a model that explains all the variation in the response variable around its mean.
Usually, the larger the R2, the better the regression model fits your observations.

### Adjusted R-squared:
The adjusted R-squared is a modified version of R-squared that adjusts for predictors. when we compare Adj-R-Squared among the input variables, sa lower adjusted R-squared indicates that the additional input variables are not adding value to the model. A higher adjusted R-squared indicates that the changes in input variables are adding value to the model.s

### Residuals:
Residuals are estimates of experimental error obtained by subtracting the observed responses from the predicted responses. The predicted response is calculated from the chosen model. Since this is a form of error, the same general assumptions apply to the group of residuals that we typically use for errors in general: one expects them to be (roughly) normal and (approximately) independently distributed with a mean of 0 and some constant variance.in other words, we should not see any pattern in the residuals when plotting. A simple plot is suitable for displaying the normality of the distribution of a group of residuals

### p-value:
we need to evaluate p-value and if the p-value is much less than 0.05 then we can reject the null hypothesis. That will indicate that there is a significant relationship between the variables in the linear regression model of the data set faithful.

## Model comparison
Now, before we select our model, let's find out the statistics of our models so that we can compare the models using the statistics i.e. R2, MSE, F-statistic, Number of Variables (K), Number of Observations (N), and number of observations in the original training set that were excluded from the model. 

```{r echo=FALSE, message=FALSE}
results = NULL
models = list(model1 = lmodel1, 
              model2 = lmodel2,
              model3 = lmodel3)

for (i in names(models)){
  s = summary(models[[i]])
  name = i
  mse = mean(s$residuals^2)
  r2 = s$r.squared
  pvalue = s$coefficients[[8]] 
  adjustedR2 = s$adj.r.squared
  f = s$fstatistic[1]
  k = s$fstatistic[2]
  n = s$fstatistic[3]
  removedObservations = nrow(train1)-n
  results = rbind(results, data.frame(name = name, 
                                      rsquared = r2,
                                      adjustedR2  = adjustedR2,
                                      mse = mse,
                                      f = f,
                                      pvalue = pvalue,
                                      k = k,
                                      n = n
                                      )
                  )
}

rownames(results) = NULL
knitr::kable(results)
```

From the table above, the r-Squared of Model2 and Model3 are almost similar. The adjusted R2 is little higher in the model 3. In addition the p-value is significant in all three models but the Model3 has lowest p-value. 
Based on the statistics and diagnostics, we can select Model3 for our final model.

## Final Model Review

Let's take a look to some other aspects of our final model.

Our final model is Model3(lmodel3): 

And the dependents columns are:
```{r}
print(names(train3))
```

Let's review the diagnostic plots and a plot of the residuals.  

```{r, echo=FALSE, message=FALSE, warning=FALSE}
plot(lmodel3)
```

### Plot the residual of selected model

```{r, echo=FALSE, message=FALSE, warning=FALSE}
plot(lmodel3$residuals)
```
From this plot, we can conclude that there is no obvious pattern in this plot.

### Create histogram of the residuals of selected model
```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data = train3, aes(x = lmodel3$residuals)) +
    geom_histogram(fill = 'steelblue', color = 'black') +
    labs(title = 'Histogram of Residuals', x = 'Residuals', y = 'Frequency')
```

From this histogram we can visualize that the distribution is pretty normal. 

### Plot the top Coefficients of our model

```{r, echo=FALSE, message=FALSE}
coef <- data.frame(sort(lmodel3$coefficients))
coef$names <- rownames(coef)
names(coef) <- c("coef","names")
imp_coef <- rbind(head(coef,10),
                  tail(coef,10))
imp_coef$coef <- scale(imp_coef$coef)
ggplot(imp_coef) +
    geom_bar(aes(x=reorder(names,coef),y=coef),
             stat="identity") +
    coord_flip() +
    ggtitle("Most Important 20 Coefficents \n in our Final Model (Scaled)") +
    theme(axis.title=element_blank(), plot.title=element_text(hjust=0.5))
```

# Predictions using evaluation data

Let's use our evaluation data to predict and evaluate our selected model.
 
```{r predict, echo=FALSE, message=FALSE, warning=FALSE}
eval_data <- read.csv('https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW1/moneyball-evaluation-data.csv', header=T)

#create Team Batting total in the evaluation data
eval_data = createTEAM_BATTING_TOTAL(eval_data)

#Add percent column to match with the model
eval_data$TEAM_BATTING_PERCENT =  eval_data$TEAM_BATTING_TOTAL / 162

#convert header to lower case to match with the models
names(eval_data) <- tolower(names(eval_data))

predictiondf <- predict(lmodel3, eval_data)
predictiondf <- data.frame(predictiondf)

write.csv(predictiondf, "/Users/anjalibm.com/Documents/DataScience/DATA_621_Group1/HW1/testPredictions.csv")
```

## Prediction explanation will be added by Zhouxin Shi(Joe)

