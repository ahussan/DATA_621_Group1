---
title: "DATA 621 Homework 4"
author: "Critical Thinking Group 1"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: '2'
    smart: no
  word_document:
    toc: yes
    toc_depth: '2'
---
\pagebreak
\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Prof. Dr. Nasrin Khansari\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
DATA 621 – Business Analytics and Data Mining\\
\medskip
Home Work 4\\
\medskip
Prepared by:\\
\medskip
Critical Thinking Group  1\\ 
\medskip
Vic Chan\\ 
\smallskip
Gehad Gad\\
\smallskip
Evan McLaughlin\\  
\smallskip
Bruno de Melo\\
\smallskip
Anjal Hussan\\
\smallskip
Zhouxin Shi\\
\smallskip
Sie Siong Wong\\
\end{center}

\pagebreak


```{r message=FALSE, warning=FALSE, echo=FALSE}
if (!require('ggplot2')) (install.packages('ggplot2'))
if(!require('dplyr')) (install.packages('dplyr'))
if(!require('tidyverse')) (install.packages('tidyverse'))
if(!require('purrr')) (install.packages('purrr'))
if(!require('mice')) (install.packages('mice'))
if(!require('DataExplorer')) (install.packages('DataExplorer'))
if(!require('MASS')) (install.packages('MASS'))
if(!require('caret')) (install.packages('caret'))
if(!require('stats')) (install.packages('stats'))
if(!require('randomForest')) (install.packages('randomForest'))
```


\newpage

# Introduction

In this homework assignment, we will explore, analyze and model a data set containing approximately 8000 records representing a customer at an auto insurance company. Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.


# Statement of the Problem

The purpose of this report is to build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car. 

# Data Exploration  

```{r echo=FALSE}
Train_Data <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW4/insurance_training_data.csv")
Eval_Data <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW4/insurance-evaluation-data.csv")
```


Let's take a look in the structure of our train data set - excluding the first column **index** which it is not to be used.


```{r echo=FALSE}
Train_Data <- Train_Data[,-1]
str(Train_Data)
```
We can see that the training data has 8161 observations(rows) and 25 variables (columns). Of these 25 columns, many are of factors type but were imported as characters or doubles - there will be properly converted in the preparation section. Also, there may be some ordinal levels within some of the factors. 

Below we display a summary of each feature.


```{r echo=FALSE}
#Summary of each feature
summary(Train_Data) 
```

We can observe the followings:

**KIDSDRIV**: Max is 4

**AGE**: age is 16 is the youngest and oldest 81. There are 6 NA values

**HOMEKIDS**: Max is 5

**TRAVTIME**: 75% of the population is below 44 but the Max value is 142. It looks like there may be some outliers here. 

**TIF**: The majority of people are not long time customers

**CLM_FREQ**: Maximum is over 5 years

**MVR_PTS**: 75% have 3 or less, maximum is 13

**CAR_AGE**: Strange!. The minimum -3 and Max is 28. There are 510 NA values. These negative values will have to be excluded from the analysis.

**INCOME** - **BLUEBOOK** - **HOME_VAL** - **OLDCLAIM** : These are numerical variables that need to be converted accordingly.


## Convertion to numerical

As can be seen below, these four features are now corrected represented.

```{r echo=FALSE}
Train_Data$INCOME <- gsub(",","",(Train_Data$INCOME))
Train_Data$INCOME <- sub('.', '', Train_Data$INCOME)
Train_Data$INCOME <-trimws(Train_Data$INCOME, which = c("both"), whitespace = "[ \t\r\n]")
Train_Data$INCOME <- as.numeric(Train_Data$INCOME)
Train_Data$HOME_VAL <- gsub(",","",(Train_Data$HOME_VAL))
Train_Data$HOME_VAL <- sub('.', '', Train_Data$HOME_VAL)
Train_Data$HOME_VAL <-trimws(Train_Data$HOME_VAL, which = c("both"), whitespace = "[ \t\r\n]")
Train_Data$HOME_VAL <- as.numeric(Train_Data$HOME_VAL)
# 
Train_Data$BLUEBOOK <- gsub(",","",(Train_Data$BLUEBOOK))
Train_Data$BLUEBOOK <- sub('.', '', Train_Data$BLUEBOOK)
Train_Data$BLUEBOOK <-trimws(Train_Data$BLUEBOOK, which = c("both"), whitespace = "[ \t\r\n]")
Train_Data$BLUEBOOK <- as.numeric(Train_Data$BLUEBOOK)
# 
Train_Data$OLDCLAIM <- gsub(",","",(Train_Data$OLDCLAIM))
Train_Data$OLDCLAIM <- sub('.', '', Train_Data$OLDCLAIM)
Train_Data$OLDCLAIM <-trimws(Train_Data$OLDCLAIM, which = c("both"), whitespace = "[ \t\r\n]")
Train_Data$OLDCLAIM <- as.numeric(Train_Data$OLDCLAIM)
# 
summary(Train_Data[,c(7,9,16,20)])
```

## Missing Values

```{r echo=FALSE}
#missing values by column
colSums(is.na(Train_Data)) 
```

There are missing values in several variables for a total of 1,879 NA's or about 1% of the total dataset.

## Univariate Distribution - Histograms

Below the numeric feature distributions are displayed.

```{r message=FALSE, warning=FALSE, echo=FALSE}
Train_Data1<-select_if(Train_Data, is.numeric)
Train_Data1 %>%
  keep(is.numeric) %>%                     
  tidyr::gather() %>%                            
  ggplot(aes(value)) +                     
    facet_wrap(~ key, scales = "free") +  
    geom_density()  
```
We can see that AGE, BLUEBOOK, CAR_AGE, HOME_VAL, INCOME, TRAVTIME and YOJ resemblance somewhat a normal distribution while CLM_FREQ, HOMEKIDS, KIDSDRIV, MVR_PTS, OLDCLAIM, TARGET_AMT, TIF resemblance either a binomial or Poisson distribution.

Some of the variables present a lot of zeros which could be explained as lack of data, and as such should be excluded, for example in HOMEVAL, while in others they are a rightful part of the distribution, and should be considered in the analysis, such as in INCOME, CLM_FREQ, HOMEKIDS, KIDSDRV, MVR_PTS,  OLDCLAIM, etc.


For the categorical features, we will displayed their distribution using bar charts.


```{r message=FALSE, warning=FALSE, echo=FALSE}
Train_Data %>% select(!is.numeric) %>%
  tidyr::gather() %>%                            
  ggplot(aes(value)) +                     
    facet_wrap(~ key, scales = "free") +  
   geom_bar() + coord_flip()  
```

### Correlation matrix

```{r, echo=FALSE}
plot_correlation(Train_Data)
```
```{r, echo=FALSE}
pairs(Train_Data1 %>% select_if(is.numeric))
```
# Data Preparation

## Adjusting Variables

Looking at the plots we see we have to make a few changes to some variables. We'll make HOMEKIDS boolean instead of a factor. For the rows where CAR_AGE are labled numbers, we make 0. For blank JOBS we label those as "Unknown". Finally, change Education to 1 if PhD and Masters.

```{r Homekids}
Train_Data$HOMEKIDS[Train_Data$HOMEKIDS != 0 ] <- 1
Eval_Data$HOMEKIDS[Eval_Data$HOMEKIDS != 0 ] <- 1
```

```{r CarAge}
Train_Data$CAR_AGE[Train_Data$CAR_AGE < 0 ] <- 0
Eval_Data$CAR_AGE[Eval_Data$CAR_AGE < 0 ] <- 0
```

```{r JOB}
Train_Data$JOB <- as.character(Train_Data$JOB)
Train_Data$JOB[Train_Data$JOB == ""] <- "Unknown"
Train_Data$JOB <- as.factor(Train_Data$JOB)
Eval_Data$JOB <- as.character(Eval_Data$JOB)
Eval_Data$JOB[Eval_Data$JOB == ""] <- "Unknown"
Eval_Data$JOB <- as.factor(Eval_Data$JOB)
```

```{r Education}
Train_Data$EDUCATION <- ifelse(Train_Data$EDUCATION %in% c("PhD", "Masters"), 0, 1)
```

## Missing Data

We have missing data for income, yoj, home_val, and car_age variables.

```{r}
summary(Train_Data)
plot_missing(Train_Data)
```

We assume the missing data are Missing at Random and choose to impute. The reason we want to impute the missing data rather than replacing with mean or median because of large number of missing values. If we're replacing with mean or median on the large number of missing values, can result in loss of variation in data. We're imputing the missing data using the MICE package. The method of predictive mean matching (PMM) is selected for continuous variables.

```{r}
impute_train_data <- mice(Train_Data, m=5, maxit=20, method='pmm', seed=321, print = FALSE)
densityplot(impute_train_data) 
complete_train_data <- complete(impute_train_data,2)

```

# Build Models

### Model 1 - All Variables

Our first model will seek to create a baseline using binary response variable, using a logistic regression model that contains all of our features. 

```{r}
m1b <- glm(factor(TARGET_FLAG) ~ ., family=binomial, data=subset(complete_train_data, select=-c(TARGET_AMT)))
summary(m1b)
```
The AIC result from the binomial model can be derived using the logit link function. 

### Model 2 - Backward Step Model

We will now build a model using backwards selection in order to compare if using backwards selection is better than hand picking values to create a model

In order to create the the backward step model we will be using the *MASS* package which includes the *stepAIC* function. The backward step requires us to pass a model which contains all of the predictors. Then the function will fit all the models which contains all but one of the predictors and will then pick the best model using AIC

```{r}
m2b <- stepAIC(m1b, direction='backward', trace=FALSE)
summary(m2b)
```

### Model 3 - Forward Step Model

We can use the same stepAIC function to build the third model. The forward selection approach starts from the null model and adds a variable that improves the model the most, one at a time, until the stopping criterion is met. We can see the result is different compared to the backward selection approach. The AIC is a little higher.

```{r}
m3b <- stepAIC(m1b, direction='forward', trace=FALSE)
summary(m3b)
```

### Model 4 - Stepwise Step Model

We also can use the same stepAIC function to build the fourth model using stepwise regression. The stepwise regression method involves adding or removing potential explanatory variables in succession and testing for statistical significance after each iteration. This is exactly same result as the backward step model. 

```{r}
m4b <- stepAIC(m1b, direction='both', trace=FALSE)
summary(m4b)
```

The analysis of deviance table shows further confirms that dropping these statistical insignificant variables  {*AGE*, *SEC*, *EDUCATION*, *RED_CAR*} either in model 2 or 4.

```{r}
anova(m4b,m1b, test="Chi")
```

Judging by AIC, the stepwise or backward approach reduces the dimensionality and improves fit, given its lower estimated prediction error. This suggests that, in addition to being a simple model, the stepwise or backward method works better to create an overall better fit to the data.

We next ran some multivariate regression models, using the features to predict the numeric response variable, TARGET_AMT, which gives the costs associated with a car's accident, creating a multiple linear regression model to predict the response variable.

### Model #5

```{r}
Train_MV <- Train_Data[Train_Data$TARGET_FLAG == 1,]
Train_Data$TARGET_FLAG <- NULL
Split_MV <- caret::createDataPartition(Train_MV$TARGET_AMT, p=0.85, list = F)
Cut_Train_MV <- Train_MV[Split_MV, ]
Val_Check_MV <- Train_MV[ -Split_MV, ]
MV_1 <- train(Train_MV$TARGET_FLAG ~., data = Cut_Train_MV, method = "lm",
                 trControl = trainControl(method = "cv", number = 10,
                                                     savePredictions = TRUE),
                tuneLength = 5, preProcess = c("center", "scale"))
summary(MV_1)
```
The R2 shows that the model is not a significant fit to the data, but we can try to enhance it for Model #5 using the stepwise technique again. 

### Model #6

Using Model 4 as a starting point, here we describe the results of a backwards stepwise by AIC multiple linear regression model selection process.
```{r}
MV_2 <- lm(TARGET_AMT ~ ., data = Cut_Train_MV)
MV2_step <- stepAIC(MV_2, trace = F)
summary(MV2_step)
```
While the R2 figure still demonstrates limited value in the model, we've achieved a p-value that could be considered significant. Nevertheless, the multivariate models haven't demonstrated much value, all in all. 

# Model Selection

## Multi Linear Regression
We will be looking at all the models created and looking at the metrics like R Squared Value, RMSE, F-Statistics, and Residual Plots in order to determine which is the best model which represents our data. We will then compare the best model selected against the evaluation data set in order to see if the model truly represents the dataset

```{r}
m1.summary = summary(MV1)
m2.summary = summary(MV2_step)
````
