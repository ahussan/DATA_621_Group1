---
title: "DATA 621 Homework 4"
author: "Critical Thinking Group 1"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: '2'
    smart: no
  word_document:
    toc: yes
    toc_depth: '2'
---
\pagebreak
\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Prof. Dr. Nasrin Khansari\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
DATA 621 – Business Analytics and Data Mining\\
\medskip
Home Work 3\\
\medskip
Prepared by:\\
\medskip
Critical Thinking Group  1\\ 
\medskip
Vic Chan\\ 
\smallskip
Gehad Gad\\
\smallskip
Evan McLaughlin\\  
\smallskip
Bruno de Melo\\
\smallskip
Anjal Hussan\\
\smallskip
Zhouxin Shi\\
\smallskip
Sie Siong Wong\\
\end{center}

\pagebreak


```{r message=FALSE, warning=FALSE, echo=FALSE}
if (!require('ggplot2')) (install.packages('ggplot2'))
if(!require('dplyr')) (install.packages('dplyr'))
if(!require('tidyr')) (install.packages('tidyr'))
if(!require('purrr')) (install.packages('purrr'))
if(!require('DataExplorer')) (install.packages('DataExplorer'))
if(!require('corrgram')) (install.packages("corrgram"))
if(!require('corrplot')) (install.packages("corrplot"))
if(!require('fastDummies')) (install.packages("fastDummies"))
if(!require('fitdistrplus')) (install.packages("fitdistrplus"))
if(!require('ggcorrplot')) (install.packages("ggcorrplot"))
library(tidyverse)
library(DataExplorer)
library(fitdistrplus)
library(ggcorrplot)
library(fastDummies)
```


\newpage

# Overview

In this homework assignment, we will explore, analyze and model a data set containing approximately 8000 records representing a customer at an auto insurance company. Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.


# Statement of the Problem

The purpose of this report is to build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car. 

# Data Exploration  

```{r echo=FALSE}

Train_Data <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW4/insurance_training_data.csv")
Eval_Data <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW4/insurance-evaluation-data.csv")
```


Let's take a look in the structure of our train data set - excluding the first column **index** which it is not to be used. Evaluation data set structure is similar to the train data set and will go through same 


```{r echo=FALSE}
Train_Data <- Train_Data[,-1]
str(Train_Data)

Eval_Data <- Eval_Data[,-1]


```
We can see that the training data has 8161 observations(rows) and 25 variables (columns). Of these 25 columns, many are of factors type but were imported as characters or doubles - there will be properly converted in the preparation section. Also, there may be some ordinal levels within some of the factors. 

Below we display a summary of each feature.


```{r echo=FALSE}
#Summary of each feature

summary(Train_Data) 
```

We can observe the followings:

**KIDSDRIV**: Max is 4

**AGE**: age is 16 is the youngest and oldest 81. There are 6 NA values

**HOMEKIDS**: Max is 5

**TRAVTIME**: 75% of the population is below 44 but the Max value is 142. It looks like there may be some outliers here. 

**TIF**: The majority of people are not long time customers

**CLM_FREQ**: Maximum is over 5 years

**MVR_PTS**: 75% have 3 or less, maximum is 13

**CAR_AGE**: Strange!. The minimum -3 and Max is 28. There are 510 NA values. These negative values will have to be excluded from the analysis.

**INCOME** - **BLUEBOOK** - **HOME_VAL** - **OLDCLAIM** : These are numerical variables that need to be converted accordingly.


##Convertion to numerical

As can be seen below, these four features are now corrected represented.

```{r echo=FALSE}
Train_Data$INCOME <- gsub(",","",(Train_Data$INCOME))
Train_Data$INCOME <- sub('.', '', Train_Data$INCOME)
Train_Data$INCOME <-trimws(Train_Data$INCOME, which = c("both"), whitespace = "[ \t\r\n]")
Train_Data$INCOME <- as.numeric(Train_Data$INCOME)

Train_Data$HOME_VAL <- gsub(",","",(Train_Data$HOME_VAL))
Train_Data$HOME_VAL <- sub('.', '', Train_Data$HOME_VAL)
Train_Data$HOME_VAL <-trimws(Train_Data$HOME_VAL, which = c("both"), whitespace = "[ \t\r\n]")
Train_Data$HOME_VAL <- as.numeric(Train_Data$HOME_VAL)
# 
Train_Data$BLUEBOOK <- gsub(",","",(Train_Data$BLUEBOOK))
Train_Data$BLUEBOOK <- sub('.', '', Train_Data$BLUEBOOK)
Train_Data$BLUEBOOK <-trimws(Train_Data$BLUEBOOK, which = c("both"), whitespace = "[ \t\r\n]")
Train_Data$BLUEBOOK <- as.numeric(Train_Data$BLUEBOOK)
# 
Train_Data$OLDCLAIM <- gsub(",","",(Train_Data$OLDCLAIM))
Train_Data$OLDCLAIM <- sub('.', '', Train_Data$OLDCLAIM)
Train_Data$OLDCLAIM <-trimws(Train_Data$OLDCLAIM, which = c("both"), whitespace = "[ \t\r\n]")
Train_Data$OLDCLAIM <- as.numeric(Train_Data$OLDCLAIM)
# 

Eval_Data$INCOME <- gsub(",","",(Eval_Data$INCOME))
Eval_Data$INCOME <- sub('.', '', Eval_Data$INCOME)
Eval_Data$INCOME <-trimws(Eval_Data$INCOME, which = c("both"), whitespace = "[ \t\r\n]")
Eval_Data$INCOME <- as.numeric(Eval_Data$INCOME)

Eval_Data$HOME_VAL <- gsub(",","",(Eval_Data$HOME_VAL))
Eval_Data$HOME_VAL <- sub('.', '', Eval_Data$HOME_VAL)
Eval_Data$HOME_VAL <-trimws(Eval_Data$HOME_VAL, which = c("both"), whitespace = "[ \t\r\n]")
Eval_Data$HOME_VAL <- as.numeric(Eval_Data$HOME_VAL)
# 
Eval_Data$BLUEBOOK <- gsub(",","",(Eval_Data$BLUEBOOK))
Eval_Data$BLUEBOOK <- sub('.', '', Eval_Data$BLUEBOOK)
Eval_Data$BLUEBOOK <-trimws(Eval_Data$BLUEBOOK, which = c("both"), whitespace = "[ \t\r\n]")
Eval_Data$BLUEBOOK <- as.numeric(Eval_Data$BLUEBOOK)
# 
Eval_Data$OLDCLAIM <- gsub(",","",(Eval_Data$OLDCLAIM))
Eval_Data$OLDCLAIM <- sub('.', '', Eval_Data$OLDCLAIM)
Eval_Data$OLDCLAIM <-trimws(Eval_Data$OLDCLAIM, which = c("both"), whitespace = "[ \t\r\n]")
Eval_Data$OLDCLAIM <- as.numeric(Eval_Data$OLDCLAIM)
#



summary(Train_Data[,c(7,9,16,20)])
```

##Missing Values

```{r echo=FALSE}
#missing values by column
colSums(is.na(Train_Data)) 
```

There are missing values in several variables for a total of 1,879 NA's or about 1% of the total dataset.

##Univariate Distribution - Histograms

Below the numeric feature distributions are displayed.

```{r message=FALSE, warning=FALSE, echo=FALSE}
Train_Data1<-select_if(Train_Data, is.numeric)
Train_Data1 %>%
  keep(is.numeric) %>%                     
  gather() %>%                            
  ggplot(aes(value)) +                     
    facet_wrap(~ key, scales = "free") +  
    geom_density()  

```
We can see that AGE, BLUEBOOK, CAR_AGE, HOME_VAL, INCOME, TRAVTIME and YOJ resemblance somewhat a normal distribution while CLM_FREQ, HOMEKIDS, KIDSDRIV, MVR_PTS, OLDCLAIM, TARGET_AMT, TIF resemblance either a binomial or Poisson distribution.

Let's investigate using a qq_plot:

```{r message=FALSE, warning=FALSE, echo=FALSE}
Train_Data1<-select_if(Train_Data, is.numeric)
Train_Data1 %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(sample=value)) +                     
    facet_wrap(~ key, scales = "free") +  
      stat_qq(distribution = stats::qnorm) +
      stat_qq_line(distribution = stats::qnorm)
```
In order to descriptive the distribution, we used function 'descdist' from package 'fitdistrplus'. We display one output for illustrative purposes - on featue **KIDSDRIV**, and results for all other features are shown below:


```{r message=FALSE, warning=FALSE, echo=FALSE}
  descdist(Train_Data1[,3], discrete = TRUE)

```

We can observe the followings:

**AGE**: normal distribution

*BLUEBOOK**: quasi-normal/lognormal - skewed distribution with heavy tails

**CAR_AGE**: quasi-normal/lognormal - skewed distribution with high frequency of <1, including negative.

**CLM_FREQ**: not normal - poisson type

**HOME_VAL**: quasi-normal - skewed distribution with heavy tails

**HOMEKIDS*: Beta distribution

**INCOME**:  quasi-normal - skewed distribution with heavy tails

**KIDSDRIV**: Negative binomial / Poisson

**MVR_PTS**: Beta distribution

**TARGET_AMT**: Gamma distribution

**TIF**: Poisson distribution

**TRAVTIME**: quasi-normal - skewed distribution with heavy tails

**YOJ**: normal distribution with  heavy tail

**OLDCLAIM**: Poisson distribution

**CLM_FREQ**: Beta distribution


Some of the variables present a lot of zeros which could be explained as lack of data, and as such should be excluded, for example in HOMEVAL, while in others they are a rightful part of the distribution, and should be considered in the analysis, such as in INCOME, CLM_FREQ, HOMEKIDS, KIDSDRV, MVR_PTS,  OLDCLAIM, etc.


For the categorical features, we will displayed their distribution using bar charts.


```{r message=FALSE, warning=FALSE, echo=FALSE}
Train_Data2<-Train_Data %>% select_if(negate(is.numeric))
Train_Data2 %>% 
  gather() %>%                            
  ggplot(aes(value)) +                     
    facet_wrap(~ key, scales = "free", nrow = 4) +  
   geom_bar() + coord_flip() 
```
Some of the features have several sub-categories, like **CAR_TYPE**, **EDUCATION**, and **JOB**, while the other features are binary in nature. Interaction between these sub-categories and the continous variables to be taken into consideration while building models.

##Correlation matrix

Considering the number of variables and sub-categories within the discrete features, the correlation matrix visualization is challenging. We will then show two matrices one with numeric only and other with discrete variable. Analysis are based on the whole dataset, though.


We also ran 'pairs' a function that produces a matrix of scatterplots - not displayed here due to size.


```{r, echo=FALSE}

# par(mfcol = 1:2)
model.matrix(~0+., Train_Data1) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)

```

```{r, echo=FALSE, fig.height = 28, fig.width = 20}
model.matrix(~0+., Train_Data2) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)

```

Some observations from the above charts:

- positive correlation:    
Income and HomeVal    
Income and BlueBook    
SexF and CarType SUV      
Phd Degree and Job as Doctor 
Master's Degree and Job as Lawyer
Income and Education
Income and Urbanicity Urban
      
- Negative correlation:       
Age and HomeKids     
HomeKids and CarAge   
Urbanicity Rural and Claim frequency
Urbanicity Rural and BlueBook     


##Evaluation dataset
Procedures described above were also applied to the evaluation set.


