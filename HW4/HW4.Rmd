---
title: "DATA 621 Homework 4"
author: "Critical Thinking Group 1"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: '2'
    smart: no
  word_document:
    toc: yes
    toc_depth: '2'
---
\pagebreak
\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Prof. Dr. Nasrin Khansari\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
DATA 621 – Business Analytics and Data Mining\\
\medskip
Home Work 3\\
\medskip
Prepared by:\\
\medskip
Critical Thinking Group  1\\ 
\medskip
Vic Chan\\ 
\smallskip
Gehad Gad\\
\smallskip
Evan McLaughlin\\  
\smallskip
Bruno de Melo\\
\smallskip
Anjal Hussan\\
\smallskip
Zhouxin Shi\\
\smallskip
Sie Siong Wong\\
\end{center}

\pagebreak


```{r message=FALSE, warning=FALSE, echo=FALSE}
if (!require('ggplot2')) (install.packages('ggplot2'))
if(!require('dplyr')) (install.packages('dplyr'))
if(!require('tidyr')) (install.packages('tidyr'))
if(!require('purrr')) (install.packages('purrr'))
if(!require('mice')) (install.packages('mice'))
if(!require('DataExplorer')) (install.packages('DataExplorer'))
library(DataExplorer)

```


\newpage

# Overview

In this homework assignment, we will explore, analyze and model a data set containing approximately 8000 records representing a customer at an auto insurance company. Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.


# Statement of the Problem

The purpose of this report is to build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car. 

# Data Exploration  

```{r echo=FALSE}

Train_Data <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW4/insurance_training_data.csv")
Eval_Data <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW4/insurance-evaluation-data.csv")
```


Let's take a look in the structure of our train data set - excluding the first column **index** which it is not to be used.


```{r echo=FALSE}
Train_Data <- Train_Data[,-1]
str(Train_Data)
```
We can see that the training data has 8161 observations(rows) and 25 variables (columns). Of these 25 columns, many are of factors type but were imported as characters or doubles - there will be properly converted in the preparation section. Also, there may be some ordinal levels within some of the factors. 

Below we display a summary of each feature.


```{r echo=FALSE}
#Summary of each feature

summary(Train_Data) 
```

We can observe the followings:

**KIDSDRIV**: Max is 4

**AGE**: age is 16 is the youngest and oldest 81. There are 6 NA values

**HOMEKIDS**: Max is 5

**TRAVTIME**: 75% of the population is below 44 but the Max value is 142. It looks like there may be some outliers here. 

**TIF**: The majority of people are not long time customers

**CLM_FREQ**: Maximum is over 5 years

**MVR_PTS**: 75% have 3 or less, maximum is 13

**CAR_AGE**: Strange!. The minimum -3 and Max is 28. There are 510 NA values. These negative values will have to be excluded from the analysis.

**INCOME** - **BLUEBOOK** - **HOME_VAL** - **OLDCLAIM** : These are numerical variables that need to be converted accordingly.


##Convertion to numerical

As can be seen below, these four features are now corrected represented.

```{r echo=FALSE}
Train_Data$INCOME <- gsub(",","",(Train_Data$INCOME))
Train_Data$INCOME <- sub('.', '', Train_Data$INCOME)
Train_Data$INCOME <-trimws(Train_Data$INCOME, which = c("both"), whitespace = "[ \t\r\n]")
Train_Data$INCOME <- as.numeric(Train_Data$INCOME)

Train_Data$HOME_VAL <- gsub(",","",(Train_Data$HOME_VAL))
Train_Data$HOME_VAL <- sub('.', '', Train_Data$HOME_VAL)
Train_Data$HOME_VAL <-trimws(Train_Data$HOME_VAL, which = c("both"), whitespace = "[ \t\r\n]")
Train_Data$HOME_VAL <- as.numeric(Train_Data$HOME_VAL)
# 
Train_Data$BLUEBOOK <- gsub(",","",(Train_Data$BLUEBOOK))
Train_Data$BLUEBOOK <- sub('.', '', Train_Data$BLUEBOOK)
Train_Data$BLUEBOOK <-trimws(Train_Data$BLUEBOOK, which = c("both"), whitespace = "[ \t\r\n]")
Train_Data$BLUEBOOK <- as.numeric(Train_Data$BLUEBOOK)
# 
Train_Data$OLDCLAIM <- gsub(",","",(Train_Data$OLDCLAIM))
Train_Data$OLDCLAIM <- sub('.', '', Train_Data$OLDCLAIM)
Train_Data$OLDCLAIM <-trimws(Train_Data$OLDCLAIM, which = c("both"), whitespace = "[ \t\r\n]")
Train_Data$OLDCLAIM <- as.numeric(Train_Data$OLDCLAIM)
# 
summary(Train_Data[,c(7,9,16,20)])
```

##Missing Values

```{r echo=FALSE}
#missing values by column
colSums(is.na(Train_Data)) 
```

There are missing values in several variables for a total of 1,879 NA's or about 1% of the total dataset.

##Univariate Distribution - Histograms

Below the numeric feature distributions are displayed.

```{r message=FALSE, warning=FALSE, echo=FALSE}
Train_Data1<-select_if(Train_Data, is.numeric)
Train_Data1 %>%
  keep(is.numeric) %>%                     
  gather() %>%                            
  ggplot(aes(value)) +                     
    facet_wrap(~ key, scales = "free") +  
    geom_density()  

```
We can see that AGE, BLUEBOOK, CAR_AGE, HOME_VAL, INCOME, TRAVTIME and YOJ resemblance somewhat a normal distribution while CLM_FREQ, HOMEKIDS, KIDSDRIV, MVR_PTS, OLDCLAIM, TARGET_AMT, TIF resemblance either a binomial or Poisson distribution.

Some of the variables present a lot of zeros which could be explained as lack of data, and as such should be excluded, for example in HOMEVAL, while in others they are a rightful part of the distribution, and should be considered in the analysis, such as in INCOME, CLM_FREQ, HOMEKIDS, KIDSDRV, MVR_PTS,  OLDCLAIM, etc.


For the categorical features, we will displayed their distribution using bar charts.


```{r message=FALSE, warning=FALSE, echo=FALSE}
Train_Data %>% select(!is.numeric) %>%
  gather() %>%                            
  ggplot(aes(value)) +                     
    facet_wrap(~ key, scales = "free") +  
   geom_bar() + coord_flip()  
```

### Correlation matrix

```{r, echo=FALSE}
plot_correlation(Train_Data)
```
```{r, echo=FALSE}
pairs(Train_Data1 %>% select_if(is.numeric))
```
# 2. DATA PREPARATION  
## Adjusting Variables  
Looking at the plots we see we have to make a few changes to some variables. We'll make HOMEKIDS boolean instead of a factor. For the rows where CAR_AGE are labled numbers, we make 0. For blank JOBS we label those as "Unknown". Finally, change Education to 1 if PhD and Masters.
```{r Homekids}
Train_Data$HOMEKIDS[Train_Data$HOMEKIDS != 0 ] <- 1
Eval_Data$HOMEKIDS[Eval_Data$HOMEKIDS != 0 ] <- 1
```  
```{r CarAge}
Train_Data$CAR_AGE[Train_Data$CAR_AGE < 0 ] <- 0
Eval_Data$CAR_AGE[Eval_Data$CAR_AGE < 0 ] <- 0
```  
```{r JOB}
Train_Data$JOB <- as.character(Train_Data$JOB)
Train_Data$JOB[Train_Data$JOB == ""] <- "Unknown"
Train_Data$JOB <- as.factor(Train_Data$JOB)
Eval_Data$JOB <- as.character(Eval_Data$JOB)
Eval_Data$JOB[Eval_Data$JOB == ""] <- "Unknown"
Eval_Data$JOB <- as.factor(Eval_Data$JOB)
```
```{r Education}
Train_Data$EDUCATION <- ifelse(Train_Data$EDUCATION %in% c("PhD", "Masters"), 0, 1)
```
## Missing Data  
We have missing data in income, yoj, home_val, and car_age. 
```{r}
summary(Train_Data)
plot_missing(Train_Data)
```  
## Imputate  
Let's runmice imputation on both the train and eval_data set.  
```{r}
mice_imputes <- mice(Train_Data, m = 2, maxit = 2, print = FALSE)
densityplot(mice_imputes)  
```  
```{r}
m_train <- median(Train_Data$AGE, na.rm = T)
Train_Data$AGE[is.na(Train_Data$AGE)] <- m_train
m_test <- median(Train_Data$AGE, na.rm = T)
Train_Data$AGE[is.na(Train_Data$AGE)] <- m_test
mice_train <-  mice(Train_Data, m = 1, maxit = 1, print = FALSE)
Train_Data <- complete(mice_train)
mice_test <- mice(Eval_Data, m = 1, maxit = 1, print = FALSE)
Eval_Data <- complete(mice_test)
```  
## Building Models
```{r}
Train_Data$TARGET_AMT <- NULL
split <- caret::createDataPartition(Train_Data$TARGET_FLAG, p=0.85, list=FALSE)
Cut_Train <- Train_Data[split, ]
Val_Check <- Train_Data[ -split, ]
```

### Model #1
Our first model will seek to create a baseline using binary response variable, using a logistic regression model that contains all of our features. 
```{r}
Binary1 <- glm(TARGET_FLAG~., family=binomial, data=Cut_Train)
summary(Binary1)
```
The AIC result from the binomial model can be derived using the logit link function. 

### Model #2
This model will be derived using a stepwise approach to determine which is more significant. We also attempt to identify impacts from multicollinearity.

```{r}
Binar_step <- step(Binary1)
summary(Binary_step)
```
Judging by AIC, the stepwise approach reduces the dimensionality and improves fit, given its lower estimated prediction error. This suggests that, in addition to being a simple model, the stepwise method works better to create an overall better fit to the data.  

### Model #3
Another method for binary modeling includes a random forest model that uses the same set of variables from the previous two models.

```{r}
RF <- randomForest::randomForest(TARGET_FLAG~., data=Cut_Train)
```

We next ran some multivariate regression models, using the features to predict the numeric response variable, TARGET_AMT, which gives the costs associated with a car's accident, creating a multiple linear regression model to predict the response variable.

### Model #4

```{r}
Train_MV <- Train_Data [Train_Data$TARGET_FLAG == 1, ]
Train_Data$TARGET_FLAG <- NULL
Split_MV <- caret::createDataPartition(Train_MV$TARGET_AMT, p=0.85, list = F)
Cut_Train_MV <- Train_MV[Split_MV, ]
Val_Check_MV <- Train_MV[ -Split_MV, ]

MV_1 <- train(Train_MV$TARGET_FLAG ~., data = Cut_Train_MV, method = "lm",
                 trControl = trainControl(method = "cv", number = 10,
                                                     savePredictions = TRUE),
                tuneLength = 5, preProcess = c("center", "scale"))
summary(MV1)

```
The R2 shows that the model is not a significant fit to the data, but we can try to enhance it for Model #5 using the stepwise technique again. 

### Model #5

Using Model 4 as a starting point, here we describe the results of a backwards stepwise by AIC multiple linear regression model selection process.
```{r}
MV_2 <- lm(TARGET_AMT ~ ., data = Cut_Train_MV)
MV2_step <- stepAIC(MV_2, trace = F)

summary(MV2_step)

```
While the R2 figure still demonstrates limited value in the model, we've achieved a p-value that could be considered significant. Nevertheless, the multivariate models haven't demonstrated much value, all in all. 


