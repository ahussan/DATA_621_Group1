---
title: "DATA 621 Final Project"
author: "Critical Thinking Group 1"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: '2'
    smart: no
  word_document:
    toc: yes
    toc_depth: '2'
---
\pagebreak
\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Prof. Dr. Nasrin Khansari\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
DATA 621 – Business Analytics and Data Mining\\
\medskip
Final Project\\
\medskip
Prepared by:\\
\medskip
Critical Thinking Group  1\\ 
\medskip
Vic Chan\\ 
\smallskip
Gehad Gad\\
\smallskip
Evan McLaughlin\\  
\smallskip
Bruno de Melo\\
\smallskip
Anjal Hussan\\
\smallskip
Zhouxin Shi\\
\smallskip
Sie Siong Wong\\
\end{center}

\pagebreak


```{r message=FALSE, warning=FALSE, echo=FALSE}
if(!require('tidyverse')) (install.packages('tidyverse'))
if(!require('caret')) (install.packages('caret'))
if(!require('ggpubr')) (install.packages('ggpubr'))
if(!require('DataExplorer')) (install.packages('DataExplorer'))
if(!require('rstatix')) (install.packages('rstatix'))
if(!require('corrplot')) (install.packages('corrplot'))
if(!require('RColorBrewer')) (install.packages('RColorBrewer'))
if(!require('randomForest')) (install.packages('randomForest'))
if(!require('caTools')) (install.packages('caTools'))
if(!require('tree')) (install.packages('tree'))
if(!require('car')) (install.packages('car'))
library(randomForest)
library(caTools)
```


\newpage

# Abstract
For the final project we will be using different predictive modeling techniques that we have learned throughout the course and using real world data in order to predict if a person has heart disease based on certain factors. The dataset for this final project [Heart Failure Prediction Dataset](https://www.kaggle.com/fedesoriano/heart-failure-prediction) can be found on Kaggle. Each observation in this dataset represents a person's health history, including their age, sex, cholesterol levels, etc. The dataset includes a total of 918 distinct individuals gathered from different countries and agencies. This dataset includes 12 different features, of categorical and/or continous values, of an individual health record, including if the individual has heart disease or not.

# Background
Cardiovascular diseases is the number 1 cause of death globally. The WHO records show that cardiovascular diseases accounted for 31% of all deaths worldwide in 2016. According to the CDC, nearly 6.2 million adults in the United States suffer heart failure, and in 2018 alone, heart failure was mentioned on 379,800 death certificates. Also, the treatment cost of health care services and medicines is costly and estimated about 31 billion in 2012. Early detection and management for heart disease could be effective in reducing the incidence of heart failure. 

Note that heart failure occurs when the heart cannot pump enough blood and oxygen to support other organs in the body. With this dataset, we would like to see if we can develop a good model to predict if a person has heart disease and what factors can be attributed to heart disease most directly. We will be tackling this question with the usage of different regression techniques and algorithms learned from this class.

# Introduction
The reason why we have chosen this dataset to work with is because of how relavent this dataset is towards the real world. Heart disease is a condition which affects everyone and being able to successfully predict heart failure ahead of time can save millions of lives. Data scientist are studying everyday trying to understand the predictors of heart failure which is why datasets like this are often on Kaggle in order to gather data scientist from all over to help solve this issue in order to help society.

# Literature Review
Several papers have been written about heard heart failure detection using different statistical methodologies - especially related to classification using machine learning techniques.

An initial search in Google scholar site underhttps://scholar.google.com/scholar?hl=en&as_sdt=0%2C33&q=diagnosis+of+heart+disease&btnG=&oq=diagnosis+of+hear retuns around 4min paper! Just in 2021, google accounts for 80k papers and citations.

Given the depth of this study field, we decided to cite one paper that uses a similar dataset but uses prediction techniques not learned in this class: Prediction of heart disease and classifiers’ sensitivity analysis by Khaled Mohamad Almustafa published at BMC Bioinformatics volume 21, Article number: 278 (2020): https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-03626-y#Abs1 
This paper also offers an ample reference of other related paper that could be beneficial for future related work in this area.

In the paper, a comparative analysis of different classifiers was performed for the classification of the Heart Disease dataset in order to correctly classify and or predict HD cases with minimal attributes. The algorithms used K- Nearest Neighbor (K-NN), Naive Bayes, Decision tree J48, JRip, SVM, Adaboost, Stochastic Gradient Decent (SGD) and Decision Table (DT) classifiers to show the performance of the selected classifications algorithms to best classify, and or predict, the HD cases.

Result is that using different classification algorithms for the classification of the HD dataset gives very promising results in term of the classification accuracy for the K-NN (K=1), Decision tree J48 and JRip classifiers with accuracy of classification of 99.7073, 98.0488 and 97.2683% respectively.

Main conclusion is to have a reliable feature selection method for HD disease prediction with using minimal number of attributes instead of having to consider all available ones.


# Methodolgy
Methodology will follows typical data science project: from understanding the dataset through exploratory data analysis, data preparation, model buildings and finally model evaluation. 


# Data Exploration

## Attribute Information

**Age**: age of the patient [years]
**Sex**: sex of the patient [M: Male, F: Female]
**ChestPainType**: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]
**RestingBP**: resting blood pressure [mm Hg]
**Cholesterol**: serum cholesterol [mm/dl]
**FastingBS**: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]
**RestingECG**: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]
**MaxHR**: maximum heart rate achieved [Numeric value between 60 and 202]
**ExerciseAngina**: exercise-induced angina [Y: Yes, N: No]
**Oldpeak**: oldpeak = ST [Numeric value measured in depression]
**ST_Slope**: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]
**HeartDisease**: output class [1: heart disease, 0: Normal]

## Split Dataset

We split the dataset by allocating 70% of the dataset to the training set and 30% to the test set. The training set will be used to build models and the evaluation set will be used to evaluate the performance of the models. 

```{r, echo=FALSE, warning=FALSE}
set.seed(123)
dataset = read.csv('https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/Final/heart.csv')
train_index <- createDataPartition(dataset$HeartDisease, p = .7, list = FALSE, times = 1)
train_data = dataset[train_index,]
eval_data = dataset[-train_index,]
```

This dataset contain 918 observations and 12 variables. There are 6 integer variables, 5 character variables, and 1 numeric variable.

The response variable will be the **HeartDisease** and the rest of 11 variables are all predictors.

```{r, echo=FALSE, warning=FALSE}
str(dataset)
```

Below we'll display a few basic EDA techniques to gain insight into our heart failure dataset.

## Summary Statistic

We can see from quite a few of variables are high variance and skewed. Also, there are no missing data in this dataset. 

```{r, echo=FALSE, warning=FALSE}
dataset %>% dplyr::select(-c(Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope)) %>%  psych::describe()
plot_missing(dataset)
```

From below histogram and density plots, we notice that there are some zero values for Cholesterol variable. As Cholesterol measurement cannot be zero, we may need to want to consider replace with mean or median or impute the zeros.

```{r, echo=FALSE, warning=FALSE}
plot_histogram(dataset)
plot_density(dataset)
```

For predictors which are character data type, we do count for categorical for which having heart disease. From below bar plots, we can notice Male is more likely to get heart disease than women. Also, most heart disease patients do not feel chest pain, normal blood pressure, having exercise-induced angina, and flat st segment. 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
sex <- dataset %>% dplyr::select(c(Sex, HeartDisease)) %>% filter(HeartDisease ==1) %>% group_by(HeartDisease, Sex) %>% summarise(count=n()) %>% ggplot(aes(x=reorder(Sex, -count), y=count, fill=Sex)) + geom_bar(stat="identity") + ggtitle("Sex Count of Heart Disease = 1") + theme(legend.position = "none", axis.title.x = element_blank(), axis.text.x = element_text(angle = 0, vjust = 0.5, hjust=1))

chestpain <- dataset %>% dplyr::select(c(ChestPainType, HeartDisease)) %>% filter(HeartDisease ==1) %>% group_by(HeartDisease, ChestPainType) %>% summarise(count=n()) %>% ggplot(aes(x=reorder(ChestPainType, -count), y=count, fill=ChestPainType)) + geom_bar(stat="identity") + ggtitle("Chest Pain Type Count of Heart Disease = 1") + theme(legend.position = "none", axis.title.x = element_blank(), axis.text.x = element_text(angle = 0, vjust = 0.5, hjust=1)) 

resting <- dataset %>% dplyr::select(c(RestingECG, HeartDisease)) %>% filter(HeartDisease ==1) %>% group_by(HeartDisease, RestingECG) %>% summarise(count=n()) %>% ggplot(aes(x=reorder(RestingECG, -count), y=count, fill=RestingECG)) + geom_bar(stat="identity") + ggtitle("Resting Blood Pressure Count of Heart Disease = 1") + theme(legend.position = "none", axis.title.x = element_blank(), axis.text.x = element_text(angle = 0, vjust = 0.5, hjust=1)) 

exercise <- dataset %>% dplyr::select(c(ExerciseAngina, HeartDisease)) %>% filter(HeartDisease ==1) %>% group_by(HeartDisease, ExerciseAngina) %>% summarise(count=n()) %>% ggplot(aes(x=reorder(ExerciseAngina, -count), y=count, fill= ExerciseAngina)) + geom_bar(stat="identity") + ggtitle("Exercise-induced Angina Count of Heart Disease = 1") + theme(legend.position = "none", axis.title.x = element_blank(), axis.text.x = element_text(angle = 0, vjust = 0.5, hjust=1))

slope <- dataset %>% dplyr::select(c(ST_Slope, HeartDisease)) %>% filter(HeartDisease ==1) %>% group_by(HeartDisease, ST_Slope) %>% summarise(count=n()) %>% ggplot(aes(x=reorder(ST_Slope, -count), y=count, fill=ST_Slope)) + geom_bar(stat="identity") + ggtitle("Slope of Peak Exercise ST Segment Count of Heart Disease = 1") + theme(legend.position = "none", axis.title.x = element_blank(), axis.text.x = element_text(angle = 0, vjust = 0.5, hjust=1))

diabetic <- dataset %>% dplyr::select(c(FastingBS, HeartDisease)) %>% filter(HeartDisease ==1) %>% group_by(HeartDisease, FastingBS) %>% summarise(count=n()) %>% ggplot(aes(x=reorder(FastingBS, -count), y=count, fill=FastingBS)) + geom_bar(stat="identity") + ggtitle("Diabetic Individuals Count of Heart Disease = 1") + theme(legend.position = "none", axis.title.x = element_blank(), axis.text.x = element_text(angle = 0, vjust = 0.5, hjust=1))




ggarrange(sex, chestpain, resting, exercise, slope, diabetic,
          ncol = 2, nrow = 3)
```

## Identifying Multicollinearity

The correlation chart below shows some correlation among predictors. We can do a correlation test for each continuous predictor by using the Pearson method and at 95% confidence level to determine if any correlation among predictors is significant.

```{r, echo=FALSE, warning=FALSE}
correlation = cor(dplyr::select(dataset,-c(Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope)), use = 'pairwise.complete.obs')
corrplot(correlation, method='ellipse', type = 'lower', order = 'hclust',col=brewer.pal(n=8, name="RdYlBu"))
```
From the table below, we can see most pairwise combinations' p-value are less than 5% significant level. This indicates those pairwise combinations are significantly correlated. Note that the effect of collinearity on prediction is less serious. The accuracy of the prediction depends on the distance from the observed data. Collinear data only covers very small fraction of the the predictor space.

```{r, echo=FALSE, warning=FALSE}
chol <- dataset %>% cor_test(vars="Cholesterol",vars2=c("MaxHR","FastingBS","RestingBP","Age","Oldpeak"), method="pearson")
maxhr <- dataset %>% cor_test(vars="MaxHR",vars2=c("Cholesterol","FastingBS","RestingBP","Age","Oldpeak"), method="pearson")
fastbs <- dataset %>% cor_test(vars="FastingBS",vars2=c("Cholesterol","MaxHR","RestingBP","Age","Oldpeak"), method="pearson")
restbp <- dataset %>% cor_test(vars="RestingBP",vars2=c("Cholesterol","MaxHR","FastingBS","Age","Oldpeak"), method="pearson")
age <- dataset %>% cor_test(vars="Age",vars2=c("Cholesterol","MaxHR","FastingBS","RestingBP","Oldpeak"), method="pearson")
oldp <- dataset %>% cor_test(vars="Oldpeak",vars2=c("Cholesterol","MaxHR","FastingBS","RestingBP","Age"), method="pearson")
combine_df <- bind_rows(chol, maxhr, fastbs, restbp, age, oldp) %>% select(c(var1,var2, p))
combine_df[order(-combine_df$p),]
```

# Data Preparation
In this section we will be looking at different ways to prepare the data for modeling and will be showing the different steps and reasons.

To understand the data in a better way, we created the following table to explain the Definition and the value of each feature.

```{r echo=FALSE}
Feature <- c("Age", "Sex", "ChestPainType","RestingBP", "Cholesterol", "FastingBS", "RestingECG", "MaxHR", "ExerciseAngina", "Oldpeak", "ST_Slope", "HeartDisease" )
 
Definition <- c("Patient's age in years", "Gender", "Type of chest-pain", "Resting blood pressure in mmHg","Serum cholestoral in mg/dl", "Fasting blood sugar higher than 120 mg/dl", "Resting electrocardiographic results", "Maximum heart rate achieved", "Exercise induced angina", "ST depression induced by exercise relative to rest", "The slope of the peak exercise ST segment", 
"Diagnosis of heart disease")


Value <- c("28 - 77", "(0)female, (1)male", "(0)typical angina, (1)atypical angina, (2)non-angina pain, (3)asymptomatic", "0 - 200", "0 – 603", "(0)False (1)True", "(0)normal, (1)having ST-T wave abnormality,  (2)showing probable left ventricular hypertrophy", "60 –202", ("(0)No(1)Yes"), ("-2.6 - 6.2") ,  "(1)upsloping, (2)flat, (3)downsloping", 
"(0)heart disease not present, (1)heart disease present")

df <- data.frame(Feature, Definition, Value)
knitr::kable(df)
```

```{r}
str(train_data)
```

The 'str' function describes the structure of the data. We need to change/convert most of the features.

```{r}
#Change values to 0 and 1
#Sex
train_data$Sex<-ifelse(train_data$Sex=="M",1,0)
#ExerciseAngina
train_data$ExerciseAngina <- ifelse(train_data$ExerciseAngina == "Y", 1,0)

```

```{r}
#ChestPainType: Change typical angina (TA) to 0, atypical angina (ATA) to 1, non-angina pain(NAP) to 2, asymptomatic(ASY) to 3.
train_data$ChestPainType = factor(train_data$ChestPainType, levels = c('TA','ATA','NAP','ASY'), labels = c('0','1','2','3'))

#RestingECG: 0 for Normal, 1 for ST, and 2 for LVH
train_data$RestingECG = factor(train_data$RestingECG, levels = c('Normal','ST','LVH'), labels = c('0','1','2'))

#ST_Slope: 0 for UP, 1 for FLAT, and 2 for DOWN
train_data$ST_Slope = factor(train_data$ST_Slope, levels = c('Up','Flat','Down'), labels = c('0','1','2'))
```

```{r}
#Convert columns to factor
train_data$Sex <- as.factor(train_data$Sex)
train_data$ExerciseAngina <- as.factor(train_data$ExerciseAngina)
train_data$FastingBS <- as.factor(train_data$FastingBS)
train_data$HeartDisease <- as.factor(train_data$HeartDisease)

#Convert columns to num
train_data$RestingBP <- as.numeric(train_data$RestingBP)
train_data$Age <- as.numeric(train_data$Age)
train_data$Cholesterol <- as.numeric(train_data$Cholesterol)
train_data$MaxHR <- as.numeric(train_data$MaxHR)
```

## Distribution of Heart Disease
Next we will be looking at how many data points do we have where the client has or does not have heart disease. We would like to make sure that the dataset is has a balance of people with and without heart disease in order to create an accurate model.

```{r}
train_data %>%
  count(HeartDisease)
```

As we can see from the result the dataset is also balanced as we have 410 datapoints which dont have heart disease and 508 data points with heart disease. While the dataset might seem balanced on the surface when we look further we will see that the dataset is not balanced based on gender

## Distribution of Heart Disease among males and females

```{r}
xtabs(~ HeartDisease + Sex, data= train_data)
```
There are 50 females out of 193 who have diagnosed with heart disease and 458 males out of 725 were diagnosed with heart disease.

This indicates that 63% of males in this dataset are diagnosed with heart disease where is only 26% of females are diagnosed with heart disease.

**We can conclude that males are more diagnosed with heart disease than females**

```{r}
mosaicplot(train_data$Sex ~ train_data$HeartDisease,
           main="Heart disease outcome by Gender", shade=FALSE,color=TRUE,
           xlab="Gender", ylab="Heart disease")
```

**Numerical Variables**

```{r}
#Create a subset with numerical data
Numerical <- train_data  %>%
  select(Age,Oldpeak, MaxHR, Cholesterol, RestingBP,HeartDisease) %>% 
  gather(key = "key", value = "value", -HeartDisease)
```

```{r}
#Visualize numeric variables using boxplots
Numerical %>% 
  ggplot(aes(y = value)) +
       geom_boxplot(aes(fill = HeartDisease),
                      alpha  = .6,
                      fatten = .7) +
        labs(x = "",
             y = "",
             title = "Boxplots for Numeric Variables") +
      scale_fill_manual(
            values = c("#fde725ff", "#20a486ff"),
            name   = "Heart\nDisease",
            labels = c("No HD", "Yes HD")) +
      theme(
         axis.text.x  = element_blank(),
         axis.ticks.x = element_blank()) +
      facet_wrap(~ key, 
                 scales = "free", 
                 ncol   = 2) 
```

**Categorical Variables**

```{r}
#Create a subset for the categorical variables
Categorical <- train_data  %>%
  select(Sex, ChestPainType, FastingBS, RestingECG, ST_Slope,ExerciseAngina, HeartDisease) %>%
  gather(key = "key", value = "value", -HeartDisease)
```


```{r}

#Visualize with bar plot
Categorical %>% 
  ggplot(aes(value)) +
    geom_bar(aes(x        = value, 
                 fill     = HeartDisease), 
                 alpha    = .6, 
                 position = "dodge", 
                 color    = "black",
                 width    = .8
             ) +
    labs(x = "",
         y = "",
         title = "barplot for Categorical Variables") +
    theme(
         axis.text.y  = element_blank(),
         axis.ticks.y = element_blank()) +
    facet_wrap(~ key, scales = "free", nrow = 4) +
    scale_fill_manual(
         values = c("#fde725ff", "#20a486ff"),
         name   = "Heart\nDisease",
         labels = c("No diagnosis", " Diagnosed"))
```

## Missing Data
This dataset seems to not have any missing data which is quite rare for a dataset. This means that there is not going to be a need to remove the NA rows or to replace the NA items with the median or mean. But while the dataset may not have any missing data there might be other issues that needs to be addressed
```{r}
na_count = sapply(train_data, function(y) sum(is.na(y)))
na_count = data.frame(na_count)
na_count
```

## Duplicate Row
We will be wanting to remove all duplicate rows in the data set in order to make sure that we will not be skewing results when creating the model
```{r}
train_data = train_data %>% distinct()
```

It seems like that our dataset does not have any duplicates as we have the same amount of rows that we started with.

## Outliers
As seen in the data exploration part of the section we can see that we have a lot of data values with 0 for **Cholesterol** and one data point with 0 **Resting BP**. As we know it is imposible for humans to have 0 **Cholesterol** or 0 **Resting BP** so we will be needing to fix this in the dataset. We will be using the median to fix all the zero values as the median is less susceptible  to Outliers. We will compare results with a dataset where 0 values were excluded from the set.

### Cholesterol
Minimun values is now:
```{r}
#remove rows where cholesterol & restingBP == 0 and create new dataset
train_data1 <- train_data
train_data1 <- train_data1[train_data1$Cholesterol != 0,]
train_data1 <- train_data1[train_data1$RestingBP != 0,]

#replace 0 with median
cholesterol_median <- median(train_data$Cholesterol[train_data$Cholesterol!= 0])
train_data$Cholesterol[train_data$Cholesterol == 0] <- cholesterol_median
min(train_data$Cholesterol)
```

### Resting BP
Minimun values is now:
```{r}
rest_median <- median(train_data$RestingBP[train_data$RestingBP!= 0])
train_data$RestingBP[train_data$RestingBP == 0] <- rest_median
min(train_data$RestingBP)
```


# Model Creation
We can group the features in four different categories:
- Physical attributes: *age*; *sex*
- General Health: *restingBP*; *Cholesterol*; *FastingBS*
- ECG related results: *RestingECG*; *MaxHR*; *Oldpeak*; *ST_Slope*; 
- Symptomatic: *ChestPainType*; *ExerciseAngina*

Considering that the response variable is binary, we start with a logistic regression with all features included - this is our base model. We also run the same model on the smaller dataset where 0 values were excluded from the set.

```{r}
model1<-glm(HeartDisease~. , family=binomial(link="logit"), data=train_data)
summary(model1)

model1a<-glm(HeartDisease~. , family=binomial(link="logit"), data=train_data1)

```
It's surprising to see that neither 'cholesterol', 'age' nor 'blood pressure' have any significance in the base model. We calculate the variance inflation factor (VIF) to check for multicollinearity - no evidence is found thereof.

```{r}
vif(model1)
```


Removing non-significant terms with the help of function `drop1', we fit a reduced model - model2.

```{r}
drop1(model1,test="Chi")
model2<-glm(HeartDisease ~ Sex + ChestPainType + FastingBS + MaxHR + ExerciseAngina + Oldpeak + ST_Slope, family = binomial(link = "logit"), data = train_data)
model2a<-glm(HeartDisease ~ Sex + ChestPainType + FastingBS + MaxHR + ExerciseAngina + Oldpeak + ST_Slope, family = binomial(link = "logit"), data = train_data1)

summary(model2)
```

Next, we build a model using variables in the physical attributes and general health groups which are commonly associated with heart diseases: cholesterol, blood pressure, age and genre.


```{r}
model3<-glm(HeartDisease~ Cholesterol + Sex + RestingBP + Age + FastingBS, family=binomial(link="logit"), data=train_data)
model3a<-glm(HeartDisease~ Cholesterol + Sex + RestingBP + Age + FastingBS, family=binomial(link="logit"), data=train_data1)
summary(model3)
```
We see that 'cholesterol' and 'age' are now significant when other variables are excluded. Only 'RestingBP' is not significant. We will then fit a model removing this feature.

```{r}
model4<-glm(HeartDisease~ Cholesterol + Sex +  Age + FastingBS, family=binomial(link="logit"), data=train_data)
model4a<-glm(HeartDisease~ Cholesterol + Sex + Age + FastingBS, family=binomial(link="logit"), data=train_data1)
summary(model4)
```


Next, we build a model using variables in the other categories: ECG related results and symptomatic.

```{r}
model5<-glm(HeartDisease~  MaxHR + RestingECG + Oldpeak + ST_Slope+
              ChestPainType + ExerciseAngina, family=binomial(link="logit"), data=train_data)
model5a<-glm(HeartDisease~ MaxHR + RestingECG + Oldpeak + ST_Slope+
              ChestPainType + ExerciseAngina, family=binomial(link="logit"), data=train_data1)
summary(model5)
```
We now fit a reduced model with only significant features.

```{r}
model6<-glm(HeartDisease~  MaxHR +  Oldpeak + ST_Slope+
              ChestPainType, family=binomial(link="logit"), data=train_data)
model6a<-glm(HeartDisease~ MaxHR +  Oldpeak + ST_Slope+
              ChestPainType , family=binomial(link="logit"), data=train_data1)
summary(model6)
```
We now fit a model using Random Forest algorithm for regression. Looking at the variable importance - higher value mean more important:

```{r}
set.seed(345)
model7<-randomForest(HeartDisease~. , importance=TRUE, mtry = 3, data=train_data)
round(importance(model7), 2)
# 
# model8<-tree(HeartDisease~.,data=train_data )
# plot(prune.tree(model8))
# 
# 
# reprtree:::plot.getTree(model7)
# 
# 
# plot(model8)
# 
# tree <- getTree(model7, k=1, labelVar=TRUE)
# 
# realtree <- reprtree:::as.tree(tree, model7)
# 
# plot(realtree)



```

We see that the most important variables are `ST_Slope`, `ChestPainType`, `Oldpeak`, 'Age', 'MaxHR'. 

# Model Selection

# Conclusion'
