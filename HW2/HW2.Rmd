---
title: "DATA 621 Homework 2"
author: "Critical Thinking Group 1"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: 6
  html_document:
    toc: yes
    toc_depth: '2'
    smart: no
  word_document:
    toc: yes
    toc_depth: '2'
---
\pagebreak
\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Prof. Dr. Nasrin Khansari\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
DATA 621 â€“ Business Analytics and Data Mining\\
\medskip
Home Work 2\\
\medskip
Prepared by:\\
\medskip
Critical Thinking Group  1\\ 
\medskip
Vic Chan\\ 
\smallskip
Gehad Gad\\
\smallskip
Evan McLaughlin\\  
\smallskip
Bruno de Melo\\
\smallskip
Anjal Hussan\\
\smallskip
Zhouxin Shi\\
\smallskip
Sie Siong Wong\\
\end{center}

\pagebreak


```{R message=FALSE, warning=FALSE, echo=FALSE,}
library(dplyr)
library(tidyr)
```

# Data Source 
```{r load data}

cm <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW2/classification-output-data.csv", header=TRUE, sep=",")

```

# Data Explained and Confusion Matrix
From the confusion matrix table below, we've actual class value versus predicted class value. There are 119 true negative, 27 true positive, 30 false negative, and 5 false positive.

```{r confusion matrix}

conf_mtx <- cm %>% dplyr::select(class, scored.class) %>% table()
conf_mtx

```
\pagebreak
# Function for Accuracy of Predictions 
```{r accuracy}

accuracy <- function(x){
  
  numerator <- x[2,2] + x[1,1]
  denominator <- sum(x)
  return(numerator/denominator)
}

```
\pagebreak
# Function for Classification Error Rate of Predictions 

```{r classification error rate}

error_rate <- function(x){
  
  numerator <- x[1,2] + x[2,1]
  denominator <- sum(x)
  return(numerator/denominator)
}

```


```{r verification of accuracy and error sums to one}

accuracy(conf_mtx) + error_rate(conf_mtx)

```
\pagebreak
# Function for Precisions of Predictions

```{r precision}

precision <- function(x){
  
  numerator <- x[1,1]
  denominator <- x[1,1] + x[1,2]
  return(numerator/denominator)
}

```
\pagebreak
# Function for Sensitivity of Predictions

```{r sensitivity}

sensitivity <- function(x){
  
  numerator <- x[1,1]
  denominator <- x[1,1] + x[2,1]
  return(numerator/denominator)
}

```
\pagebreak
# Function for Specificity of Predictions 

```{r specificity}

specificity <- function(x){
  
  numerator <- x[2,2]
  denominator <- x[2,2] + x[1,2]
  return(numerator/denominator)
}

```
\pagebreak
# F1 Score of Predictions 

```{r f1 score}

f1_score <- function(x){
  
  numerator <- 2 * precision(x) * sensitivity(x)
  denominator <- precision(x) + sensitivity(x)
  return(numerator/denominator)
}

```


\pagebreak

# Investigation of the `pROC` R package. 

We used the `pROC` R package to generate an ROC curve for the data set. 

```{r warning=FALSE, message=FALSE, echo=FALSE}
library(pROC)
```
```{r warning=FALSE, message=FALSE}
rcurve <- roc(cm$class~cm$scored.probability)
plot(rcurve, main="ROC Curve") 
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, results='asis'}
bestPROC <- coords(rcurve, "best", ret=c("threshold", "1-specificity", "sensitivity"))
```

Best Threshold value using pROC package is `r (sprintf("{Threshold = %f,fpr = %f,tpr = %f}", 
            bestPROC[1], bestPROC[2], bestPROC[3]))`

Note: The second method (using auc) predicts better than first method (using distance from (0,1))


\newpage

