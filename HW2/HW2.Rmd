---
title: "HW2"
author: "Critical Thinking Group 1"
date: "9/27/2021"
  pdf_document:
    toc: true
    number_sections: true
urlcolor: cyan
linkcolor: red
---

```{R}
library(dplyr)
library(tidyr)
```

1. 
```{r load data}
cm <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW2/classification-output-data.csv", header=TRUE, sep=",")

```

From the confusion matrix table below, we've actual class value versus predicted class value. There are 119 true negative, 27 true positive, 30 false negative, and 5 false positive.

2.
```{r confusion matrix}

conf_mtx <- cm %>% dplyr::select(class, scored.class) %>% table()
conf_mtx

```

3. 
```{r accuracy}

accuracy <- function(x){
  
  numerator <- x[2,2] + x[1,1]
  denominator <- sum(x)
  return(numerator/denominator)
}

```

4. 
```{r classification error rate}

error_rate <- function(x){
  
  numerator <- x[1,2] + x[2,1]
  denominator <- sum(x)
  return(numerator/denominator)
}

```

```{r verification of accuracy and error sums to one}

accuracy(conf_mtx) + error_rate(conf_mtx)

```

5. 
```{r precision}

precision <- function(x){
  
  numerator <- x[1,1]
  denominator <- x[1,1] + x[1,2]
  return(numerator/denominator)
}

```

6. 
```{r sensitivity}

sensitivity <- function(x){
  
  numerator <- x[1,1]
  denominator <- x[1,1] + x[2,1]
  return(numerator/denominator)
}

```

7. 
```{r specificity}

specificity <- function(x){
  
  numerator <- x[2,2]
  denominator <- x[2,2] + x[1,2]
  return(numerator/denominator)
}

```

8. 
```{r f1 score}

f1_score <- function(x){
  
  numerator <- 2 * precision(x) * sensitivity(x)
  denominator <- precision(x) + sensitivity(x)
  return(numerator/denominator)
}

```

9. We will assume that $a = Precision$ and $b = Sensitivity$. To see the bounds of the f1 score we will be setting both a and b to their maximum and minimum.

Assuming that $a = 1$ and $b = 1$ we can see that the F1 score is equal to 1. 

```{=latex}
\begin{tabular}{ll}
F1 Score = \frac{2 * 1 * 1}{1 + 1} = 1
\end{tabular}
```

If we assume that a and b are approaching zero we can see that the f1 value will be positive non zero number. Since a and b are bounded between 0 and 1 this means that the F1 function will always be between 0 and 1 which means that $ab < a$ and $ab < b$. 

```{r ROC}
fetch_accuracy <- function(df){
  dt <- df %>% dplyr::select( scored.class, class ) %>% table()
  #positive
  TP <- dt[2,2]
  #negative
  TN <- dt[1,1]
  #false positive
  FP <- dt[2,1]
  #false negative
  FN <- dt[1,2]
  
  return(round((TP + TN)/(TP + FP + TN + FN),4))
}

#write function

fetch_roc_curve <- function(x,p){
    x <- x[order(p, decreasing=TRUE)]
    
    TP = cumsum(x)/sum(x)
    FP = cumsum(!x)/sum(!x)
    
    roc_df <- data.frame(TP, FP)
    auc <- sum(TP * c(diff(FP), 0)) + sum(c(diff(TP), 0) * c(diff(FP), 0))/2
    
    return(c(df=roc_df, auc = auc))
}

#apply function to dataset

roc_data <- fetch_roc_curve(cm$class, cm$scored.probability)
plot(roc_data[[2]],roc_data[[1]], type = 'l', main = "ROC",xlab="1-Spec (FPR)", ylab = "Sens (TPR)")
abline(0,1, lty=3)
legend(0.7,0.4, round(roc_data$auc,8), title = 'AUC')

```
This curve illustrates the true positive rate versus the false positive rate and enables us to assess the accuracy. We are thus able to classify our observations through the establishment of probability thresholds. AUC is a measurement of our model's suitability for determining positive and negative outcomes. Our relatively high AUC is a good "grade" for our desire to correctly guess outcomes.

