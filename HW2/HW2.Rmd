---
title: "HW2"
author: "Critical Thinking Group 1"
date: "9/27/2021"
  pdf_document:
    toc: true
    number_sections: true
urlcolor: cyan
linkcolor: red
---

```{R}
library(dplyr)
library(tidyr)
```

```{r load data}

cm <- read.csv("C:/Users/wongs34/Documents/School/DATA 621/Homework/2/classification-output-data.csv", header=TRUE, sep=",")

```

From the confusion matrix table below, we've actual class value versus predicted class value. There are 119 true negative, 27 true positive, 30 false negative, and 5 false positive.

```{r confusion matrix}

conf_mtx <- cm %>% dplyr::select(class, scored.class) %>% table()
conf_mtx

```

```{r accuracy}

accuracy <- function(x){
  
  numerator <- x[2,2] + x[1,1]
  denominator <- sum(x)
  return(numerator/denominator)
}

```


```{r classification error rate}

error_rate <- function(x){
  
  numerator <- x[1,2] + x[2,1]
  denominator <- sum(x)
  return(numerator/denominator)
}

```


```{r verification of accuracy and error sums to one}

accuracy(conf_mtx) + error_rate(conf_mtx)

```


```{r precision}

precision <- function(x){
  
  numerator <- x[1,1]
  denominator <- x[1,1] + x[1,2]
  return(numerator/denominator)
}

```


```{r sensitivity}

sensitivity <- function(x){
  
  numerator <- x[1,1]
  denominator <- x[1,1] + x[2,1]
  return(numerator/denominator)
}

```


```{r specificity}

specificity <- function(x){
  
  numerator <- x[2,2]
  denominator <- x[2,2] + x[1,2]
  return(numerator/denominator)
}

```


```{r f1 score}

f1_score <- function(x){
  
  numerator <- 2 * precision(x) * sensitivity(x)
  denominator <- precision(x) + sensitivity(x)
  return(numerator/denominator)
}

```

```{r ROC}
fetch_accuracy <- function(df){
  dt <- df %>% dplyr::select( scored.class, class ) %>% table()
  #positive
  TP <- dt[2,2]
  #negative
  TN <- dt[1,1]
  #false positive
  FP <- dt[2,1]
  #false negative
  FN <- dt[1,2]
  
  return(round((TP + TN)/(TP + FP + TN + FN),4))
}

#write function

fetch_roc_curve <- function(x,p){
    x <- x[order(p, decreasing=TRUE)]
    
    TP = cumsum(x)/sum(x)
    FP = cumsum(!x)/sum(!x)
    
    roc_df <- data.frame(TP, FP)
    auc <- sum(TP * c(diff(FP), 0)) + sum(c(diff(TP), 0) * c(diff(FP), 0))/2
    
    return(c(df=roc_df, auc = auc))
}

#apply function to dataset

roc_data <- fetch_roc_curve(cm$class, cm$scored.probability)
plot(roc_data[[2]],roc_data[[1]], type = 'l', main = "ROC",xlab="1-Spec (FPR)", ylab = "Sens (TPR)")
abline(0,1, lty=3)
legend(0.7,0.4, round(roc_data$auc,8), title = 'AUC')

```

This curve illustrates the true positive rate versus the false positive rate and enables us to assess the accuracy. We are thus able to classify our observations through the establishment of probability thresholds. AUC is a measurement of our model's suitability for determining positive and negative outcomes. Our relatively high AUC is a good "grade" for our desire to correctly guess outcomes.
