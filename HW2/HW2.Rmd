---
title: "DATA 621 Homework 2"
author: "Critical Thinking Group 1"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: '2'
    smart: no
  word_document:
    toc: yes
    toc_depth: '2'
---
\pagebreak
\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Prof. Dr. Nasrin Khansari\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
DATA 621 â€“ Business Analytics and Data Mining\\
\medskip
Home Work 2\\
\medskip
Prepared by:\\
\medskip
Critical Thinking Group  1\\ 
\medskip
Vic Chan\\ 
\smallskip
Gehad Gad\\
\smallskip
Evan McLaughlin\\  
\smallskip
Bruno de Melo\\
\smallskip
Anjal Hussan\\
\smallskip
Zhouxin Shi\\
\smallskip
Sie Siong Wong\\
\end{center}

\pagebreak


```{R message=FALSE, warning=FALSE, echo=FALSE}
library(dplyr)
library(tidyr)
library(knitr)
library(caret)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

# 1. Data Source

```{r load data}

cm <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW2/classification-output-data.csv", header=TRUE, sep=",")

```

R markdown for this howework can be found at:
https://github.com/ahussan/DATA_621_Group1/blob/main/HW2/HW2.Rmd


# 2. Data Set and Confusion Matrix
From the confusion matrix table below, we have actual class or reference value versus predicted class value, and we assume *0* to be the non-event class and *1* to be positive class. Class values are display in columns, while predicted values are displayed in the rows. So there are 119 true negatives (TN), 27 true positives (TP), 30 false negatives (FN), and 5 false positives (FP).

```{r confusion matrix}

conf_mtx <- cm %>% dplyr::select(scored.class, class) %>% table()
conf_mtx

```

# 3. Function for Accuracy of Predictions 
```{r accuracy}

accuracy <- function(x){
  
  numerator <- x[2,2] + x[1,1]
  denominator <- sum(x)
  return(numerator/denominator)
}

```
# 4. Function for Classification Error Rate of Predictions 

```{r classification error rate}

error_rate <- function(x){
  
  numerator <- x[1,2] + x[2,1]
  denominator <- sum(x)
  return(numerator/denominator)
}

```

We verify below that `accuracy` and an `error rate` sums to one.


```{r verification of accuracy and error sums to one}

accuracy(conf_mtx) + error_rate(conf_mtx)

```

# 5. Function for Precisions of Predictions

```{r precision}


precision <- function(x){
  
  numerator <- x[2,2]
  denominator <- x[2,2] + x[2,1]
  return(numerator/denominator)
}

```

# 6. Function for Sensitivity of Predictions

```{r sensitivity}


sensitivity <- function(x){
  
  numerator <- x[2,2]
  denominator <- x[2,2] + x[1,2]
  return(numerator/denominator)
}


```

# 7. Function for Specificity of Predictions 

```{r specificity}


specificity <- function(x){
  
  numerator <- x[1,1]
  denominator <- x[1,1] + x[2,1]
  return(numerator/denominator)
}

```

# 8. F1 Score of Predictions 

```{r f1 score}

f1_score <- function(x){
  
  numerator <- 2 * precision(x) * sensitivity(x)
  denominator <- precision(x) + sensitivity(x)
  return(numerator/denominator)
}

```
\pagebreak
# 9. Bound on the F1 score
We will assume that $a = Precision$ and $b = Sensitivity$. So, we re-rewrite:

$$
F1Score = \frac{2 * a * b}{a + b}
$$

To see the bounds of the f1 score we will be setting both $a$ and $b$ to their maximum and minimum.

Assuming that $a = 1$ and $b = 1$ we can see that the F1 score is equal to 1. 

$$
F1Score = \frac{2 * 1 * 1}{1 + 1} = 1
$$

If we assume that $a$ and $b$ are approaching zero we can see that the f1 value will be positive non zero number. Since a and b are bounded between 0 and 1 this means that the F1 function will always be between 0 and 1 which means that $ab < a$ and $ab < b$. 

\pagebreak
# 10. Write a function to write a ROC curve


```{r ROC }
fetch_roc_curve <- function(x,p){
    x <- x[order(p, decreasing=TRUE)]
    
    TP = cumsum(x)/sum(x)
    FP = cumsum(!x)/sum(!x)
    
    roc_df <- data.frame(TP, FP)
    auc <- sum(TP * c(diff(FP), 0)) + sum(c(diff(TP), 0) * c(diff(FP), 0))/2
    
    plot_roc <- function() {
      plot(roc_df[[2]],roc_df[[1]], type = 'l', main = "ROC",xlab="1-Spec (FPR)", ylab = "Sens (TPR)")
      abline(0,1, lty=3)
      legend(0.7,0.4, round(auc,8),title = 'AUC')
      }
    
    return(plot_roc())
}
fetch_roc_curve(cm$class, cm$scored.probability)
```
This curve illustrates the true positive rate versus the false positive rate and enables us to assess the accuracy. We are thus able to classify our observations through the establishment of probability thresholds. AUC is a measurement of our model's suitability for determining positive and negative outcomes. Our relatively high AUC is a good "grade" for our desire to correctly guess outcomes.
       




# 11. Classification metrics output       

Table 1 below shows the results of the functions we built, applied to the confusion matrix.      

```{r  classiftable}
Value <- c(round(accuracy(conf_mtx),4), round(error_rate(conf_mtx),4), round(f1_score(conf_mtx),4),round(precision(conf_mtx),4), round(sensitivity(conf_mtx),4), round(specificity(conf_mtx),4))

names(Value) <- c("Accuracy", "Error Rate", "F1_score", "Precision", "Sensitivity", "Specificity")

kable(Value, col.names = "Score", caption = "Summary of Classification Metrics") 
```

  
\pagebreak
# 12. Investigation of the `caret` R package. 
We used the `caret` R package to calculate a confusion Matrix, sensitivity, and specificity for the data set. See below for the confusion matrix:


```{r warning=FALSE, message=FALSE}

conf_mtx_caret <- confusionMatrix(factor(cm$scored.class), factor(cm$class),  positive = '1')

conf_mtx_caret$table


```
Result from the `caret` package is similar to our calculated matrix. For two classes, the `caret` function assumes as default that the class corresponding to an event is the first class level, in our case *0*. We changed the positive argument in the function, and assume the positive class to be *1*.

```{r}
conf_mtx == conf_mtx_caret$table
```
Calling `confusionMatrix` function can be used to generate statistics.

```{r}
conf_mtx_caret
```

Using the function `byclass` argument, we can extract *sensitivity* and *specificity* measures and thus compare with our own functions.


```{r}
sens<-conf_mtx_caret$byClass[1]
spec<-conf_mtx_caret$byClass[2]

sensitivity(conf_mtx) == sens
specificity(conf_mtx) == spec

```

Overall results comparison in table 2 below:

```{r echo=FALSE}
car_value<- c(conf_mtx_caret$overall[1], 1-conf_mtx_caret$overall[1],spec<-conf_mtx_caret$byClass[7],conf_mtx_caret$byClass[5], conf_mtx_caret$byClass[1],conf_mtx_caret$byClass[2])

names(car_value)<-names(Value)  

df<-data.frame(car_value)
df<-data.frame(Value)
df[,2]<-data.frame(car_value)
colnames(df)<-c("Own", "Caret")
  
kable(df, caption = "Summary of Classification Metrics", digits = 4,  align = "c") 
```





# 13. Investigation of the `pROC` R package. 

We used the `pROC` R package to generate an ROC curve for the data set. 

```{r warning=FALSE, message=FALSE, echo=FALSE}
library(pROC)
```
```{r warning=FALSE, message=FALSE}
rcurve <- roc(cm$class~cm$scored.probability)
plot(rcurve, main="ROC Curve") 
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, results='asis'}
bestPROC <- coords(rcurve, "best", ret=c("threshold", "1-specificity", "sensitivity"))
```

Best Threshold value using pROC package is `r (sprintf("{Threshold = %f,fpr = %f,tpr = %f}", 
            bestPROC[1], bestPROC[2], bestPROC[3]))`

Note: The second method (using auc) predicts better than first method (using distance from (0,1))


\newpage

