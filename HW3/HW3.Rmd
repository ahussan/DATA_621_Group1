---
title: "DATA 621 Homework 3"
author: "Critical Thinking Group 1"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: '2'
    smart: no
  word_document:
    toc: yes
    toc_depth: '2'
---
\pagebreak
\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Prof. Dr. Nasrin Khansari\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
DATA 621 â€“ Business Analytics and Data Mining\\
\medskip
Home Work 3\\
\medskip
Prepared by:\\
\medskip
Critical Thinking Group  1\\ 
\medskip
Vic Chan\\ 
\smallskip
Gehad Gad\\
\smallskip
Evan McLaughlin\\  
\smallskip
Bruno de Melo\\
\smallskip
Anjal Hussan\\
\smallskip
Zhouxin Shi\\
\smallskip
Sie Siong Wong\\
\end{center}

\pagebreak


```{R message=FALSE, warning=FALSE, echo=FALSE}
if (!require('ggplot2')) (install.packages('ggplot2'))
if(!require('corrplot')) (install.packages("corrplot"))
if(!require('corrgram')) (install.packages("corrgram"))
if(!require('Hmisc')) (install.packages("Hmisc"))
if(!require('DataExplorer')) (install.packages("DataExplorer"))
if(!require('MASS')) (install.packages('MASS'))
```


\newpage

# Introduction

Crime has a high cost to all parts of society and it can have severe long term impact on neighborhoods. If crime rises in the neighborhood, it affects the neighborhood. Additionally, crime can even have a health cost to the community in that the perception of a dangerous neighborhood was associated with significantly lower odds of having high physical activity among both men and women. It is important to understand the propensity for crime levels of a neighborhood before investing in that neighborhood. 

# Statement of the Problem

The purpose of this report is to develop a binary logistic regression model to determine if the neighborhood will be at risk for high crime level.

# Data Exploration  

```{r load data, echo=FALSE}
train <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW3/crime-training-data_modified.csv", header=TRUE, sep=",")
```


Let's take a look to the first few rows of our train data set
```{r, echo=FALSE}
print(head(train, 10))
```
Looks like all the columns are numerical. The target variable is a binary variable indicating if the crime rate above the median rate (1) or not (0)

### Means
Column means of our train data set are as follows: 
```{r, echo=FALSE}
print(colMeans(train))

```

### Standard Deviation
Now let's take a look at the standard deviation of our predictor variables:
```{r, echo=FALSE}
print(apply(train, 2, sd))

```

### Median Value
Let's take a look at the median value of our predictor variables:
```{r, echo=FALSE}
print(apply(train, 2, median))

```

### Bar chart or box plot
```{r, echo=FALSE}
boxplot(train, use.cols = TRUE)

```

### Correlation matrix

```{r, echo=FALSE}
train.cor = cor(train)
print(train.cor)
```


```{r, echo=FALSE}
library(corrgram)
corrgram(train, order=TRUE, lower.panel=panel.shade,
  upper.panel=panel.pie, text.panel=panel.txt,
  main="visualize the data in correlation matrices ")

```

### Compare Target in Training

We make sure there are no issues with an inappropriate distribution of the target variable in our training data.

```{r, echo=FALSE, warning=FALSE}
knitr::kable(table(train$target))
```

### Histogram of Variables

```{r, echo=FALSE, warning=FALSE}
plot_histogram(train)
relationships <- train
relationships$chas <- NULL
plot_scatterplot(relationships, "target")
```
Now that we have a basic familiarity with our data, we can analyze the relationship between the numeric variables we've brought in and the target variable. We can employ boxplots and a correlation matrix to quickly analyze this, including paired plots of the numeric feature variables. 

```{r, echo=FALSE, warning=FALSE}

#convert features to factor and add a dataset feature
train$chas <- as.factor(train$chas)
train$target <- as.factor(train$target)
train$dataset <- 'train'

plotfontsize <- 8
train_int_names <- train %>% select_if(is.numeric)
int_names <- names(train_int_names)

for (i in int_names) {
  assign(paste0("var_",i), ggplot(train, aes_string(x = train$target, y = i)) + 
          geom_boxplot(color = 'steelblue', 
                       outlier.color = 'firebrick', 
                       outlier.alpha = 0.35) +
#scale_y_continuous
          labs(title = paste0(i,' vs target'), y = i, x= 'target') +
          theme_minimal() + 
          theme(
            plot.title = element_text(hjust = 0.45),
            panel.grid.major.y =  element_line(color = "grey", linetype = "dashed"),
            panel.grid.major.x = element_blank(),
            panel.grid.minor.y = element_blank(),
            panel.grid.minor.x = element_blank(),
            axis.ticks.x = element_line(color = "grey"),
            text = element_text(size=plotfontsize)
          ))
}
gridExtra::grid.arrange(var_age, var_dis, var_indus,var_lstat,
                        var_medv,var_nox,var_ptratio,var_rad, 
                        var_rm, var_tax, var_zn, nrow=4)

numeric_values <- train %>% select_if(is.numeric)
train_cor <- cor(numeric_values)
corrplot.mixed(train_cor, tl.col = 'black', tl.pos = 'lt')
```

There are a couple of items to note in the above graphics. First, in the boxplots, we observe many outliers, which could impact our regression, limiting its predictive value. Age, nox, and dis all appear to be highly correlated with our target, and numerous other features appear to have some weaker correlative relationship. Now that we've assessed the relationship between our features and the target, we can take a quick look, through our correlation matrix, at the relationship between the variables themselves. Our correlation matrix makes clear that multicollinearity is a potential issue within our observations, and we need to keep this in mind as we create and select our models.


# Data Preparation

# Build Models

### Model 1 - All Variables

First we will be creating a model with all the variables in the original dataset to create a baseline for other models. Based on the p-values results from this model we will be able to eliminate variables with large p-values

```{r}
m1 = glm(target~., data=train, family=binomial)
summary(m1)
```

We can see that variables *zn*, *nox*, *age*, *dis*, *rad*, *ptratio*, and *medv* have p values that are close to smaller than 0.05 which will be used in the next model

### Model 2 - Hand Pick Model

```{r}
m2 = glm(target ~ zn + nox + age + dis + rad + ptratio + medv, data=train, family=binomial)
summary(m2)
```

### Model 3 - Backward Step Model
We will now build a model using backwards selection in order to compare if using backwards selection is better than hand picking values to create a model
In order to create the the backward step model we will be using the *MASS* package which includes the *stepAIC* function. The backward step requires
us to pass a model which contains all of the predictors. Then the function will fit all the models which contains all but one of the predictors and
will then pick the best model using AIC
```{r}
m3 = stepAIC(m1, direction='backward', trace=FALSE)
summary(m3)
```

### Model 4 - Forward Step Model

We can use the same stepAIC function to build the fourth model. The forward selection approach starts from the null model and adds a variable that improves the model the most, one at a time, until the stopping criterion is met. We can see the result is different compared to the backward selection approach. We can see that the result is same as the saturated model m1.

```{r}
m4 = stepAIC(m1, direction='forward', trace=FALSE)
summary(m4)
```

### Model 5 - Stepwise step Model

We also can use the same stepAIC function to build the fifth model using stepwise regression. The stepwise regression method involves adding or removing potential explanatory variables in succession and testing for statistical significance after each iteration. At the very last step stepAIC as shown in the summary table has produced the optimal set of features {*zn*, *nox*, *age*, *dis*, *rad*, *ptratio*, *medv*}. This is exactly same result as the backward step model. 

```{r}
m5 = stepAIC(m1, direction='both', trace=FALSE)
summary(m5)
```

The analysis of deviance table shows further confirms that dropping these 4 variables  {*indus*, *chas*, *rm*, *lstat*} either in model 3 or 5 are statistically insignificant and can be dropped. 

```{r}

anova(m5,m1, test="Chi")

```


# Select Models

